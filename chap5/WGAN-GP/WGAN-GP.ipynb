{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 8.065693] [G loss: -0.143634]\n",
      "1 [D loss: 5.460041] [G loss: 0.121209]\n",
      "2 [D loss: 2.948807] [G loss: -0.190615]\n",
      "3 [D loss: 2.261377] [G loss: -0.145821]\n",
      "4 [D loss: 1.867373] [G loss: -0.092122]\n",
      "5 [D loss: 0.649197] [G loss: -0.117349]\n",
      "6 [D loss: 1.205548] [G loss: -0.385343]\n",
      "7 [D loss: 1.211734] [G loss: -0.355503]\n",
      "8 [D loss: 0.414350] [G loss: 0.032869]\n",
      "9 [D loss: 0.226262] [G loss: -0.329432]\n",
      "10 [D loss: 0.871910] [G loss: -0.338091]\n",
      "11 [D loss: -0.030197] [G loss: -0.376830]\n",
      "12 [D loss: -0.393622] [G loss: -0.266495]\n",
      "13 [D loss: 0.124142] [G loss: -0.516857]\n",
      "14 [D loss: -0.499120] [G loss: -0.404012]\n",
      "15 [D loss: -0.098824] [G loss: -0.285696]\n",
      "16 [D loss: -0.526324] [G loss: -0.387287]\n",
      "17 [D loss: -0.632410] [G loss: -0.689274]\n",
      "18 [D loss: -0.445735] [G loss: -0.819335]\n",
      "19 [D loss: -0.701144] [G loss: -0.706246]\n",
      "20 [D loss: -0.279532] [G loss: -0.837826]\n",
      "21 [D loss: -0.584883] [G loss: -1.053887]\n",
      "22 [D loss: -0.645141] [G loss: -0.830322]\n",
      "23 [D loss: -0.649677] [G loss: -0.780317]\n",
      "24 [D loss: -1.074283] [G loss: -1.019410]\n",
      "25 [D loss: -0.424250] [G loss: -1.041688]\n",
      "26 [D loss: -0.500934] [G loss: -1.140419]\n",
      "27 [D loss: -0.768020] [G loss: -1.307471]\n",
      "28 [D loss: -1.085829] [G loss: -1.134022]\n",
      "29 [D loss: -0.776106] [G loss: -1.315001]\n",
      "30 [D loss: -0.683006] [G loss: -1.640651]\n",
      "31 [D loss: -0.699028] [G loss: -1.647700]\n",
      "32 [D loss: -1.045425] [G loss: -1.570833]\n",
      "33 [D loss: -1.105702] [G loss: -1.478001]\n",
      "34 [D loss: -0.879328] [G loss: -1.553451]\n",
      "35 [D loss: -0.777479] [G loss: -1.434117]\n",
      "36 [D loss: -0.506157] [G loss: -1.634788]\n",
      "37 [D loss: -0.667849] [G loss: -1.479028]\n",
      "38 [D loss: -0.817099] [G loss: -1.613791]\n",
      "39 [D loss: -0.905740] [G loss: -1.806679]\n",
      "40 [D loss: -1.378720] [G loss: -1.485493]\n",
      "41 [D loss: -0.283096] [G loss: -1.874920]\n",
      "42 [D loss: -1.014500] [G loss: -1.545159]\n",
      "43 [D loss: -0.966900] [G loss: -1.950249]\n",
      "44 [D loss: -1.167012] [G loss: -2.085182]\n",
      "45 [D loss: -0.801443] [G loss: -2.152946]\n",
      "46 [D loss: -1.021698] [G loss: -2.267252]\n",
      "47 [D loss: -1.037552] [G loss: -2.281905]\n",
      "48 [D loss: -0.584329] [G loss: -2.381693]\n",
      "49 [D loss: -0.580731] [G loss: -2.427235]\n",
      "50 [D loss: -0.913180] [G loss: -2.313809]\n",
      "51 [D loss: -0.470788] [G loss: -2.633485]\n",
      "52 [D loss: -0.353730] [G loss: -2.771146]\n",
      "53 [D loss: -0.894579] [G loss: -2.538853]\n",
      "54 [D loss: -1.067887] [G loss: -2.404758]\n",
      "55 [D loss: -0.734429] [G loss: -2.917931]\n",
      "56 [D loss: -0.505070] [G loss: -2.773506]\n",
      "57 [D loss: -0.722946] [G loss: -2.866441]\n",
      "58 [D loss: -0.467792] [G loss: -2.897005]\n",
      "59 [D loss: -0.743125] [G loss: -2.899274]\n",
      "60 [D loss: -1.428664] [G loss: -2.764274]\n",
      "61 [D loss: -0.927706] [G loss: -3.127707]\n",
      "62 [D loss: -0.254033] [G loss: -3.129495]\n",
      "63 [D loss: -0.927881] [G loss: -3.077780]\n",
      "64 [D loss: -0.623219] [G loss: -3.263806]\n",
      "65 [D loss: -0.892573] [G loss: -3.190486]\n",
      "66 [D loss: -0.075694] [G loss: -3.156760]\n",
      "67 [D loss: -0.931796] [G loss: -3.022564]\n",
      "68 [D loss: -0.122812] [G loss: -3.312638]\n",
      "69 [D loss: -0.785532] [G loss: -2.789712]\n",
      "70 [D loss: -0.608187] [G loss: -3.302885]\n",
      "71 [D loss: -0.009056] [G loss: -3.014673]\n",
      "72 [D loss: -0.937218] [G loss: -2.801979]\n",
      "73 [D loss: -0.050660] [G loss: -3.209581]\n",
      "74 [D loss: -0.257942] [G loss: -3.433579]\n",
      "75 [D loss: -0.586339] [G loss: -2.765538]\n",
      "76 [D loss: -0.321978] [G loss: -3.397068]\n",
      "77 [D loss: -0.223862] [G loss: -3.313856]\n",
      "78 [D loss: -0.101579] [G loss: -3.066780]\n",
      "79 [D loss: -0.357180] [G loss: -3.125700]\n",
      "80 [D loss: -0.400308] [G loss: -2.861624]\n",
      "81 [D loss: -0.234371] [G loss: -2.829350]\n",
      "82 [D loss: -0.562931] [G loss: -2.918976]\n",
      "83 [D loss: -0.483216] [G loss: -2.548273]\n",
      "84 [D loss: -0.512505] [G loss: -2.737251]\n",
      "85 [D loss: -0.619268] [G loss: -2.538149]\n",
      "86 [D loss: -0.435723] [G loss: -2.745137]\n",
      "87 [D loss: -0.339273] [G loss: -2.778304]\n",
      "88 [D loss: -0.746437] [G loss: -2.719700]\n",
      "89 [D loss: -0.108669] [G loss: -2.461398]\n",
      "90 [D loss: -0.427316] [G loss: -2.840187]\n",
      "91 [D loss: -0.492717] [G loss: -2.391730]\n",
      "92 [D loss: -0.756367] [G loss: -2.407147]\n",
      "93 [D loss: -0.596918] [G loss: -2.572426]\n",
      "94 [D loss: -0.832600] [G loss: -2.645399]\n",
      "95 [D loss: -0.241656] [G loss: -2.610609]\n",
      "96 [D loss: -0.658516] [G loss: -2.303483]\n",
      "97 [D loss: -0.432262] [G loss: -2.364773]\n",
      "98 [D loss: -0.286257] [G loss: -2.349917]\n",
      "99 [D loss: -0.324064] [G loss: -2.586456]\n",
      "100 [D loss: -0.699471] [G loss: -2.357683]\n",
      "101 [D loss: -0.479508] [G loss: -2.400156]\n",
      "102 [D loss: -0.451566] [G loss: -2.254963]\n",
      "103 [D loss: -0.462560] [G loss: -2.388994]\n",
      "104 [D loss: -0.492102] [G loss: -2.311339]\n",
      "105 [D loss: -0.586934] [G loss: -2.394838]\n",
      "106 [D loss: -0.130220] [G loss: -2.517508]\n",
      "107 [D loss: -1.000586] [G loss: -2.000952]\n",
      "108 [D loss: -0.683297] [G loss: -2.190053]\n",
      "109 [D loss: -0.537526] [G loss: -2.389931]\n",
      "110 [D loss: -0.825879] [G loss: -1.885218]\n",
      "111 [D loss: -0.731285] [G loss: -1.774664]\n",
      "112 [D loss: -0.366907] [G loss: -1.935485]\n",
      "113 [D loss: -0.460589] [G loss: -2.301641]\n",
      "114 [D loss: -0.738052] [G loss: -2.106402]\n",
      "115 [D loss: -0.964706] [G loss: -1.748459]\n",
      "116 [D loss: -0.478407] [G loss: -2.067072]\n",
      "117 [D loss: -0.318152] [G loss: -2.019238]\n",
      "118 [D loss: -0.562736] [G loss: -2.051930]\n",
      "119 [D loss: -0.595874] [G loss: -1.892117]\n",
      "120 [D loss: -0.540646] [G loss: -2.253568]\n",
      "121 [D loss: -0.748292] [G loss: -2.041911]\n",
      "122 [D loss: -0.814865] [G loss: -2.146682]\n",
      "123 [D loss: -0.197031] [G loss: -2.244719]\n",
      "124 [D loss: -0.710485] [G loss: -1.834082]\n",
      "125 [D loss: -0.623996] [G loss: -1.762131]\n",
      "126 [D loss: -0.449202] [G loss: -2.001287]\n",
      "127 [D loss: -0.676370] [G loss: -1.887862]\n",
      "128 [D loss: -0.348335] [G loss: -2.215903]\n",
      "129 [D loss: -0.432077] [G loss: -1.944195]\n",
      "130 [D loss: -0.110586] [G loss: -1.995316]\n",
      "131 [D loss: -0.441209] [G loss: -2.240743]\n",
      "132 [D loss: -0.333500] [G loss: -1.940511]\n",
      "133 [D loss: -0.145776] [G loss: -2.403885]\n",
      "134 [D loss: -0.514832] [G loss: -2.127726]\n",
      "135 [D loss: -0.156004] [G loss: -2.162504]\n",
      "136 [D loss: -0.584216] [G loss: -2.017739]\n",
      "137 [D loss: -0.499609] [G loss: -2.134305]\n",
      "138 [D loss: -0.407719] [G loss: -2.300709]\n",
      "139 [D loss: -0.395957] [G loss: -2.254246]\n",
      "140 [D loss: -0.624951] [G loss: -2.245600]\n",
      "141 [D loss: -0.546164] [G loss: -2.177620]\n",
      "142 [D loss: -0.250229] [G loss: -2.588505]\n",
      "143 [D loss: -0.790952] [G loss: -2.351492]\n",
      "144 [D loss: -0.588579] [G loss: -2.231718]\n",
      "145 [D loss: -0.157488] [G loss: -2.076517]\n",
      "146 [D loss: -0.213782] [G loss: -2.392446]\n",
      "147 [D loss: 0.113898] [G loss: -2.199796]\n",
      "148 [D loss: -0.729041] [G loss: -2.249990]\n",
      "149 [D loss: -0.814503] [G loss: -2.247865]\n",
      "150 [D loss: -0.126682] [G loss: -2.159703]\n",
      "151 [D loss: -0.373598] [G loss: -2.516063]\n",
      "152 [D loss: -0.614398] [G loss: -2.296840]\n",
      "153 [D loss: -0.493305] [G loss: -2.289537]\n",
      "154 [D loss: -0.547622] [G loss: -2.355339]\n",
      "155 [D loss: -0.397465] [G loss: -2.301159]\n",
      "156 [D loss: -0.466158] [G loss: -2.516223]\n",
      "157 [D loss: -0.593416] [G loss: -2.280239]\n",
      "158 [D loss: -0.859693] [G loss: -2.847250]\n",
      "159 [D loss: -0.612829] [G loss: -2.487084]\n",
      "160 [D loss: -0.967904] [G loss: -2.654525]\n",
      "161 [D loss: -0.288466] [G loss: -2.786920]\n",
      "162 [D loss: -0.255751] [G loss: -2.576312]\n",
      "163 [D loss: -0.550105] [G loss: -2.354776]\n",
      "164 [D loss: -0.918739] [G loss: -2.530247]\n",
      "165 [D loss: -0.994766] [G loss: -2.676084]\n",
      "166 [D loss: -0.643294] [G loss: -2.529003]\n",
      "167 [D loss: -0.184823] [G loss: -2.499045]\n",
      "168 [D loss: 0.012169] [G loss: -2.167743]\n",
      "169 [D loss: -0.617612] [G loss: -2.514754]\n",
      "170 [D loss: -0.141559] [G loss: -2.564801]\n",
      "171 [D loss: -0.234388] [G loss: -2.385928]\n",
      "172 [D loss: -0.333108] [G loss: -2.273142]\n",
      "173 [D loss: -0.168093] [G loss: -2.580549]\n",
      "174 [D loss: -0.925406] [G loss: -2.468857]\n",
      "175 [D loss: -0.580631] [G loss: -2.453136]\n",
      "176 [D loss: -0.521368] [G loss: -2.521410]\n",
      "177 [D loss: -0.857962] [G loss: -2.383756]\n",
      "178 [D loss: -0.233660] [G loss: -2.632645]\n",
      "179 [D loss: -0.443703] [G loss: -2.383016]\n",
      "180 [D loss: -0.537546] [G loss: -2.939722]\n",
      "181 [D loss: -0.371667] [G loss: -2.716974]\n",
      "182 [D loss: -0.684811] [G loss: -2.753723]\n",
      "183 [D loss: -0.510872] [G loss: -2.515432]\n",
      "184 [D loss: -0.521863] [G loss: -2.880962]\n",
      "185 [D loss: -0.479788] [G loss: -2.686865]\n",
      "186 [D loss: -0.431981] [G loss: -2.761858]\n",
      "187 [D loss: -0.607038] [G loss: -2.714037]\n",
      "188 [D loss: -0.481540] [G loss: -2.679828]\n",
      "189 [D loss: -0.306063] [G loss: -2.796111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 [D loss: -0.331917] [G loss: -2.409389]\n",
      "191 [D loss: -0.715151] [G loss: -2.699764]\n",
      "192 [D loss: -0.300137] [G loss: -2.682667]\n",
      "193 [D loss: -0.744457] [G loss: -2.623314]\n",
      "194 [D loss: -0.142695] [G loss: -2.729800]\n",
      "195 [D loss: -0.307817] [G loss: -2.651387]\n",
      "196 [D loss: -0.442019] [G loss: -2.530608]\n",
      "197 [D loss: -0.439563] [G loss: -2.601676]\n",
      "198 [D loss: -0.670735] [G loss: -2.660851]\n",
      "199 [D loss: -0.247003] [G loss: -2.546070]\n",
      "200 [D loss: -0.720496] [G loss: -2.259384]\n",
      "201 [D loss: -0.748875] [G loss: -2.772216]\n",
      "202 [D loss: -0.101152] [G loss: -2.741434]\n",
      "203 [D loss: -0.536845] [G loss: -2.332596]\n",
      "204 [D loss: 0.007645] [G loss: -2.563962]\n",
      "205 [D loss: -0.615301] [G loss: -2.934612]\n",
      "206 [D loss: -0.636902] [G loss: -2.692423]\n",
      "207 [D loss: -0.261426] [G loss: -2.721348]\n",
      "208 [D loss: -0.722496] [G loss: -2.734046]\n",
      "209 [D loss: -0.471993] [G loss: -2.431586]\n",
      "210 [D loss: -0.527155] [G loss: -2.709997]\n",
      "211 [D loss: -0.255824] [G loss: -2.488529]\n",
      "212 [D loss: -0.434600] [G loss: -2.532229]\n",
      "213 [D loss: -0.555998] [G loss: -2.122086]\n",
      "214 [D loss: -0.638164] [G loss: -2.493849]\n",
      "215 [D loss: -0.587048] [G loss: -2.978533]\n",
      "216 [D loss: -0.314722] [G loss: -2.763440]\n",
      "217 [D loss: -0.403096] [G loss: -2.738087]\n",
      "218 [D loss: -0.383727] [G loss: -2.394980]\n",
      "219 [D loss: -0.441941] [G loss: -2.643211]\n",
      "220 [D loss: -0.501884] [G loss: -2.620010]\n",
      "221 [D loss: 0.021417] [G loss: -2.671171]\n",
      "222 [D loss: -0.731525] [G loss: -2.595569]\n",
      "223 [D loss: 0.080541] [G loss: -2.802967]\n",
      "224 [D loss: -0.601794] [G loss: -2.695946]\n",
      "225 [D loss: -0.413490] [G loss: -2.580778]\n",
      "226 [D loss: -0.235633] [G loss: -2.733873]\n",
      "227 [D loss: -0.380739] [G loss: -2.844295]\n",
      "228 [D loss: -0.069781] [G loss: -2.479343]\n",
      "229 [D loss: -0.552859] [G loss: -3.023792]\n",
      "230 [D loss: -0.356648] [G loss: -2.786983]\n",
      "231 [D loss: -0.519749] [G loss: -2.611516]\n",
      "232 [D loss: -0.372522] [G loss: -2.726825]\n",
      "233 [D loss: -0.462749] [G loss: -2.909472]\n",
      "234 [D loss: -0.323470] [G loss: -2.772168]\n",
      "235 [D loss: -0.624976] [G loss: -2.641381]\n",
      "236 [D loss: -0.408865] [G loss: -2.492283]\n",
      "237 [D loss: -0.705438] [G loss: -2.554063]\n",
      "238 [D loss: -0.924030] [G loss: -2.576586]\n",
      "239 [D loss: -0.111168] [G loss: -2.555434]\n",
      "240 [D loss: -0.605397] [G loss: -2.256999]\n",
      "241 [D loss: -0.182959] [G loss: -2.521647]\n",
      "242 [D loss: -0.391197] [G loss: -2.488287]\n",
      "243 [D loss: -0.258123] [G loss: -2.530159]\n",
      "244 [D loss: -0.348541] [G loss: -2.593615]\n",
      "245 [D loss: -0.539500] [G loss: -2.736404]\n",
      "246 [D loss: -0.478291] [G loss: -2.308822]\n",
      "247 [D loss: -0.409707] [G loss: -2.595631]\n",
      "248 [D loss: 0.024210] [G loss: -2.742642]\n",
      "249 [D loss: -0.449933] [G loss: -2.205199]\n",
      "250 [D loss: -0.209969] [G loss: -2.597398]\n",
      "251 [D loss: -0.505878] [G loss: -2.512671]\n",
      "252 [D loss: -0.308926] [G loss: -2.429146]\n",
      "253 [D loss: -0.439996] [G loss: -2.512703]\n",
      "254 [D loss: -0.249297] [G loss: -2.467836]\n",
      "255 [D loss: -0.588313] [G loss: -2.573881]\n",
      "256 [D loss: 0.018915] [G loss: -2.667660]\n",
      "257 [D loss: -0.433056] [G loss: -2.498292]\n",
      "258 [D loss: -0.164585] [G loss: -2.614457]\n",
      "259 [D loss: -0.008857] [G loss: -2.812714]\n",
      "260 [D loss: -0.329209] [G loss: -2.631111]\n",
      "261 [D loss: -0.388702] [G loss: -2.715725]\n",
      "262 [D loss: -0.328310] [G loss: -2.985292]\n",
      "263 [D loss: -0.341535] [G loss: -2.966363]\n",
      "264 [D loss: 0.049452] [G loss: -2.772553]\n",
      "265 [D loss: -0.094078] [G loss: -2.857350]\n",
      "266 [D loss: -0.334153] [G loss: -2.540579]\n",
      "267 [D loss: -0.303046] [G loss: -2.548665]\n",
      "268 [D loss: -0.376924] [G loss: -2.625274]\n",
      "269 [D loss: -0.335272] [G loss: -2.885052]\n",
      "270 [D loss: -0.040776] [G loss: -2.580349]\n",
      "271 [D loss: -0.073326] [G loss: -2.605701]\n",
      "272 [D loss: -0.444459] [G loss: -2.441190]\n",
      "273 [D loss: -0.491148] [G loss: -2.623200]\n",
      "274 [D loss: -0.468987] [G loss: -2.867191]\n",
      "275 [D loss: 0.074057] [G loss: -2.428701]\n",
      "276 [D loss: -0.198499] [G loss: -2.770397]\n",
      "277 [D loss: -0.547027] [G loss: -2.884255]\n",
      "278 [D loss: -0.081739] [G loss: -2.918809]\n",
      "279 [D loss: -0.345018] [G loss: -2.564420]\n",
      "280 [D loss: -0.227668] [G loss: -2.483816]\n",
      "281 [D loss: -0.359517] [G loss: -2.683906]\n",
      "282 [D loss: -0.495093] [G loss: -2.728858]\n",
      "283 [D loss: -0.324809] [G loss: -2.563710]\n",
      "284 [D loss: 0.198272] [G loss: -2.682638]\n",
      "285 [D loss: -0.098427] [G loss: -2.395907]\n",
      "286 [D loss: -0.251496] [G loss: -2.598567]\n",
      "287 [D loss: -0.254927] [G loss: -3.040132]\n",
      "288 [D loss: -0.278217] [G loss: -2.678931]\n",
      "289 [D loss: 0.044977] [G loss: -2.733681]\n",
      "290 [D loss: -0.042663] [G loss: -2.697158]\n",
      "291 [D loss: -0.513375] [G loss: -2.503622]\n",
      "292 [D loss: -0.655074] [G loss: -2.832220]\n",
      "293 [D loss: 0.169066] [G loss: -2.885293]\n",
      "294 [D loss: 0.011800] [G loss: -2.672941]\n",
      "295 [D loss: -0.349320] [G loss: -2.638937]\n",
      "296 [D loss: -0.163198] [G loss: -2.560604]\n",
      "297 [D loss: -0.379972] [G loss: -2.627257]\n",
      "298 [D loss: 0.137427] [G loss: -2.656519]\n",
      "299 [D loss: 0.186091] [G loss: -2.455488]\n"
     ]
    }
   ],
   "source": [
    "# Large amount of credit goes to:\n",
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# which I've used as a reference for this implementation\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(100,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                              self.wasserstein_loss,\n",
    "                                              partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(100,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.train_on_batch([imgs, noise],\n",
    "                                                                [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 1\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wgan = WGANGP()\n",
    "    wgan.train(epochs=300, batch_size=32, sample_interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
