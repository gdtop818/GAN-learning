{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              457856    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 681,089\n",
      "Trainable params: 680,449\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 1.13, acc.: 34.77%] [Q loss: 0.83] [G loss: 2.99]\n",
      "1 [D loss: 0.79, acc.: 63.67%] [Q loss: 1.13] [G loss: 3.02]\n",
      "2 [D loss: 0.69, acc.: 65.23%] [Q loss: 1.30] [G loss: 3.03]\n",
      "3 [D loss: 0.59, acc.: 68.36%] [Q loss: 1.75] [G loss: 2.85]\n",
      "4 [D loss: 0.56, acc.: 69.53%] [Q loss: 1.59] [G loss: 2.71]\n",
      "5 [D loss: 0.57, acc.: 67.58%] [Q loss: 1.63] [G loss: 2.59]\n",
      "6 [D loss: 0.62, acc.: 65.23%] [Q loss: 1.76] [G loss: 2.52]\n",
      "7 [D loss: 0.63, acc.: 67.97%] [Q loss: 1.82] [G loss: 2.69]\n",
      "8 [D loss: 0.56, acc.: 69.14%] [Q loss: 1.80] [G loss: 2.56]\n",
      "9 [D loss: 0.63, acc.: 65.23%] [Q loss: 1.60] [G loss: 2.67]\n",
      "10 [D loss: 0.60, acc.: 66.80%] [Q loss: 1.70] [G loss: 2.66]\n",
      "11 [D loss: 0.58, acc.: 67.19%] [Q loss: 1.63] [G loss: 2.68]\n",
      "12 [D loss: 0.52, acc.: 71.48%] [Q loss: 1.64] [G loss: 2.67]\n",
      "13 [D loss: 0.49, acc.: 70.70%] [Q loss: 1.79] [G loss: 2.65]\n",
      "14 [D loss: 0.46, acc.: 71.88%] [Q loss: 1.65] [G loss: 2.44]\n",
      "15 [D loss: 0.55, acc.: 67.97%] [Q loss: 1.66] [G loss: 2.58]\n",
      "16 [D loss: 0.50, acc.: 71.88%] [Q loss: 1.59] [G loss: 2.47]\n",
      "17 [D loss: 0.51, acc.: 71.09%] [Q loss: 1.67] [G loss: 2.45]\n",
      "18 [D loss: 0.49, acc.: 73.05%] [Q loss: 1.64] [G loss: 2.41]\n",
      "19 [D loss: 0.42, acc.: 73.44%] [Q loss: 1.74] [G loss: 2.53]\n",
      "20 [D loss: 0.53, acc.: 69.53%] [Q loss: 1.93] [G loss: 2.29]\n",
      "21 [D loss: 0.45, acc.: 75.00%] [Q loss: 1.89] [G loss: 2.41]\n",
      "22 [D loss: 0.42, acc.: 76.95%] [Q loss: 1.72] [G loss: 2.51]\n",
      "23 [D loss: 0.45, acc.: 73.44%] [Q loss: 1.61] [G loss: 2.46]\n",
      "24 [D loss: 0.50, acc.: 69.53%] [Q loss: 1.64] [G loss: 2.39]\n",
      "25 [D loss: 0.43, acc.: 77.73%] [Q loss: 1.70] [G loss: 2.56]\n",
      "26 [D loss: 0.41, acc.: 78.52%] [Q loss: 1.54] [G loss: 2.45]\n",
      "27 [D loss: 0.43, acc.: 76.17%] [Q loss: 1.64] [G loss: 2.22]\n",
      "28 [D loss: 0.41, acc.: 75.78%] [Q loss: 1.75] [G loss: 2.26]\n",
      "29 [D loss: 0.36, acc.: 80.86%] [Q loss: 1.79] [G loss: 2.18]\n",
      "30 [D loss: 0.49, acc.: 71.09%] [Q loss: 1.58] [G loss: 2.34]\n",
      "31 [D loss: 0.42, acc.: 76.56%] [Q loss: 1.71] [G loss: 2.34]\n",
      "32 [D loss: 0.46, acc.: 75.39%] [Q loss: 1.81] [G loss: 2.24]\n",
      "33 [D loss: 0.48, acc.: 73.83%] [Q loss: 1.82] [G loss: 2.19]\n",
      "34 [D loss: 0.41, acc.: 76.95%] [Q loss: 2.04] [G loss: 2.37]\n",
      "35 [D loss: 0.39, acc.: 80.08%] [Q loss: 1.84] [G loss: 2.23]\n",
      "36 [D loss: 0.50, acc.: 69.92%] [Q loss: 1.83] [G loss: 2.26]\n",
      "37 [D loss: 0.37, acc.: 78.91%] [Q loss: 1.77] [G loss: 2.42]\n",
      "38 [D loss: 0.44, acc.: 75.39%] [Q loss: 1.80] [G loss: 2.13]\n",
      "39 [D loss: 0.45, acc.: 73.44%] [Q loss: 1.92] [G loss: 2.13]\n",
      "40 [D loss: 0.38, acc.: 76.17%] [Q loss: 2.02] [G loss: 2.15]\n",
      "41 [D loss: 0.34, acc.: 80.86%] [Q loss: 1.90] [G loss: 2.19]\n",
      "42 [D loss: 0.38, acc.: 76.95%] [Q loss: 1.73] [G loss: 1.98]\n",
      "43 [D loss: 0.48, acc.: 73.83%] [Q loss: 1.73] [G loss: 2.19]\n",
      "44 [D loss: 0.39, acc.: 76.17%] [Q loss: 1.79] [G loss: 2.11]\n",
      "45 [D loss: 0.38, acc.: 80.08%] [Q loss: 1.84] [G loss: 2.03]\n",
      "46 [D loss: 0.34, acc.: 83.20%] [Q loss: 1.67] [G loss: 1.99]\n",
      "47 [D loss: 0.41, acc.: 75.78%] [Q loss: 1.68] [G loss: 1.98]\n",
      "48 [D loss: 0.37, acc.: 80.47%] [Q loss: 1.91] [G loss: 2.08]\n",
      "49 [D loss: 0.38, acc.: 75.39%] [Q loss: 1.92] [G loss: 2.03]\n",
      "50 [D loss: 0.43, acc.: 76.17%] [Q loss: 1.94] [G loss: 1.99]\n",
      "51 [D loss: 0.37, acc.: 78.52%] [Q loss: 1.94] [G loss: 1.85]\n",
      "52 [D loss: 0.38, acc.: 78.91%] [Q loss: 1.81] [G loss: 1.71]\n",
      "53 [D loss: 0.39, acc.: 78.12%] [Q loss: 1.89] [G loss: 1.83]\n",
      "54 [D loss: 0.39, acc.: 76.95%] [Q loss: 1.83] [G loss: 1.79]\n",
      "55 [D loss: 0.37, acc.: 78.52%] [Q loss: 1.87] [G loss: 1.59]\n",
      "56 [D loss: 0.33, acc.: 83.20%] [Q loss: 1.91] [G loss: 1.60]\n",
      "57 [D loss: 0.44, acc.: 74.22%] [Q loss: 2.00] [G loss: 1.69]\n",
      "58 [D loss: 0.38, acc.: 77.34%] [Q loss: 2.06] [G loss: 1.52]\n",
      "59 [D loss: 0.35, acc.: 80.08%] [Q loss: 1.94] [G loss: 1.46]\n",
      "60 [D loss: 0.37, acc.: 78.91%] [Q loss: 1.94] [G loss: 1.54]\n",
      "61 [D loss: 0.37, acc.: 77.34%] [Q loss: 1.90] [G loss: 1.59]\n",
      "62 [D loss: 0.42, acc.: 77.34%] [Q loss: 2.04] [G loss: 1.35]\n",
      "63 [D loss: 0.35, acc.: 82.81%] [Q loss: 1.82] [G loss: 1.40]\n",
      "64 [D loss: 0.38, acc.: 76.95%] [Q loss: 1.93] [G loss: 1.34]\n",
      "65 [D loss: 0.36, acc.: 81.64%] [Q loss: 1.89] [G loss: 1.16]\n",
      "66 [D loss: 0.36, acc.: 79.30%] [Q loss: 1.91] [G loss: 1.16]\n",
      "67 [D loss: 0.40, acc.: 73.83%] [Q loss: 2.01] [G loss: 1.23]\n",
      "68 [D loss: 0.37, acc.: 76.95%] [Q loss: 2.08] [G loss: 1.08]\n",
      "69 [D loss: 0.42, acc.: 78.12%] [Q loss: 2.06] [G loss: 0.89]\n",
      "70 [D loss: 0.36, acc.: 79.69%] [Q loss: 2.16] [G loss: 1.12]\n",
      "71 [D loss: 0.31, acc.: 85.16%] [Q loss: 2.08] [G loss: 0.94]\n",
      "72 [D loss: 0.35, acc.: 81.25%] [Q loss: 2.01] [G loss: 0.83]\n",
      "73 [D loss: 0.35, acc.: 80.86%] [Q loss: 1.96] [G loss: 1.04]\n",
      "74 [D loss: 0.36, acc.: 78.91%] [Q loss: 2.11] [G loss: 0.76]\n",
      "75 [D loss: 0.36, acc.: 78.91%] [Q loss: 2.22] [G loss: 0.77]\n",
      "76 [D loss: 0.35, acc.: 83.20%] [Q loss: 2.09] [G loss: 0.60]\n",
      "77 [D loss: 0.33, acc.: 83.20%] [Q loss: 2.23] [G loss: 0.69]\n",
      "78 [D loss: 0.33, acc.: 82.03%] [Q loss: 1.94] [G loss: 0.69]\n",
      "79 [D loss: 0.40, acc.: 77.34%] [Q loss: 2.15] [G loss: 0.50]\n",
      "80 [D loss: 0.37, acc.: 77.34%] [Q loss: 2.17] [G loss: 0.49]\n",
      "81 [D loss: 0.33, acc.: 84.77%] [Q loss: 2.10] [G loss: 0.52]\n",
      "82 [D loss: 0.39, acc.: 75.39%] [Q loss: 2.21] [G loss: 0.49]\n",
      "83 [D loss: 0.33, acc.: 82.42%] [Q loss: 2.17] [G loss: 0.53]\n",
      "84 [D loss: 0.36, acc.: 82.03%] [Q loss: 2.09] [G loss: 0.45]\n",
      "85 [D loss: 0.34, acc.: 80.47%] [Q loss: 2.12] [G loss: 0.44]\n",
      "86 [D loss: 0.32, acc.: 83.98%] [Q loss: 2.22] [G loss: 0.37]\n",
      "87 [D loss: 0.36, acc.: 79.30%] [Q loss: 2.18] [G loss: 0.42]\n",
      "88 [D loss: 0.38, acc.: 76.56%] [Q loss: 2.33] [G loss: 0.33]\n",
      "89 [D loss: 0.33, acc.: 82.03%] [Q loss: 2.28] [G loss: 0.40]\n",
      "90 [D loss: 0.35, acc.: 82.42%] [Q loss: 2.21] [G loss: 0.37]\n",
      "91 [D loss: 0.32, acc.: 83.98%] [Q loss: 2.13] [G loss: 0.32]\n",
      "92 [D loss: 0.38, acc.: 80.86%] [Q loss: 2.11] [G loss: 0.39]\n",
      "93 [D loss: 0.37, acc.: 79.69%] [Q loss: 2.16] [G loss: 0.20]\n",
      "94 [D loss: 0.33, acc.: 81.25%] [Q loss: 2.27] [G loss: 0.29]\n",
      "95 [D loss: 0.29, acc.: 85.16%] [Q loss: 2.09] [G loss: 0.34]\n",
      "96 [D loss: 0.42, acc.: 76.95%] [Q loss: 2.06] [G loss: 0.25]\n",
      "97 [D loss: 0.40, acc.: 76.17%] [Q loss: 2.35] [G loss: 0.23]\n",
      "98 [D loss: 0.30, acc.: 87.11%] [Q loss: 2.17] [G loss: 0.24]\n",
      "99 [D loss: 0.39, acc.: 76.95%] [Q loss: 2.09] [G loss: 0.25]\n",
      "100 [D loss: 0.40, acc.: 75.39%] [Q loss: 2.34] [G loss: 0.27]\n",
      "101 [D loss: 0.34, acc.: 83.59%] [Q loss: 2.36] [G loss: 0.28]\n",
      "102 [D loss: 0.44, acc.: 73.83%] [Q loss: 2.25] [G loss: 0.32]\n",
      "103 [D loss: 0.39, acc.: 76.56%] [Q loss: 2.23] [G loss: 0.23]\n",
      "104 [D loss: 0.34, acc.: 83.20%] [Q loss: 2.16] [G loss: 0.33]\n",
      "105 [D loss: 0.36, acc.: 80.86%] [Q loss: 2.14] [G loss: 0.25]\n",
      "106 [D loss: 0.45, acc.: 69.53%] [Q loss: 2.26] [G loss: 0.31]\n",
      "107 [D loss: 0.35, acc.: 82.81%] [Q loss: 2.32] [G loss: 0.26]\n",
      "108 [D loss: 0.35, acc.: 79.69%] [Q loss: 2.30] [G loss: 0.24]\n",
      "109 [D loss: 0.38, acc.: 79.30%] [Q loss: 2.19] [G loss: 0.30]\n",
      "110 [D loss: 0.36, acc.: 80.86%] [Q loss: 2.29] [G loss: 0.28]\n",
      "111 [D loss: 0.45, acc.: 70.70%] [Q loss: 2.35] [G loss: 0.27]\n",
      "112 [D loss: 0.40, acc.: 76.17%] [Q loss: 2.28] [G loss: 0.25]\n",
      "113 [D loss: 0.41, acc.: 78.12%] [Q loss: 2.29] [G loss: 0.24]\n",
      "114 [D loss: 0.37, acc.: 79.69%] [Q loss: 2.21] [G loss: 0.21]\n",
      "115 [D loss: 0.42, acc.: 76.17%] [Q loss: 2.23] [G loss: 0.30]\n",
      "116 [D loss: 0.41, acc.: 75.39%] [Q loss: 2.40] [G loss: 0.25]\n",
      "117 [D loss: 0.39, acc.: 84.38%] [Q loss: 2.26] [G loss: 0.19]\n",
      "118 [D loss: 0.40, acc.: 76.56%] [Q loss: 2.18] [G loss: 0.22]\n",
      "119 [D loss: 0.44, acc.: 74.61%] [Q loss: 2.29] [G loss: 0.20]\n",
      "120 [D loss: 0.42, acc.: 78.52%] [Q loss: 2.23] [G loss: 0.25]\n",
      "121 [D loss: 0.45, acc.: 73.44%] [Q loss: 2.24] [G loss: 0.26]\n",
      "122 [D loss: 0.40, acc.: 79.30%] [Q loss: 2.27] [G loss: 0.17]\n",
      "123 [D loss: 0.41, acc.: 76.95%] [Q loss: 2.17] [G loss: 0.21]\n",
      "124 [D loss: 0.41, acc.: 76.17%] [Q loss: 2.06] [G loss: 0.22]\n",
      "125 [D loss: 0.47, acc.: 71.09%] [Q loss: 2.23] [G loss: 0.29]\n",
      "126 [D loss: 0.42, acc.: 78.12%] [Q loss: 2.42] [G loss: 0.13]\n",
      "127 [D loss: 0.48, acc.: 74.22%] [Q loss: 2.24] [G loss: 0.29]\n",
      "128 [D loss: 0.45, acc.: 73.83%] [Q loss: 2.32] [G loss: 0.19]\n",
      "129 [D loss: 0.43, acc.: 76.56%] [Q loss: 2.16] [G loss: 0.18]\n",
      "130 [D loss: 0.50, acc.: 73.05%] [Q loss: 2.36] [G loss: 0.22]\n",
      "131 [D loss: 0.44, acc.: 75.00%] [Q loss: 2.32] [G loss: 0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 [D loss: 0.46, acc.: 76.17%] [Q loss: 2.08] [G loss: 0.26]\n",
      "133 [D loss: 0.44, acc.: 74.22%] [Q loss: 2.29] [G loss: 0.17]\n",
      "134 [D loss: 0.53, acc.: 69.92%] [Q loss: 2.28] [G loss: 0.26]\n",
      "135 [D loss: 0.45, acc.: 76.56%] [Q loss: 2.04] [G loss: 0.23]\n",
      "136 [D loss: 0.52, acc.: 69.92%] [Q loss: 2.14] [G loss: 0.25]\n",
      "137 [D loss: 0.47, acc.: 73.83%] [Q loss: 2.01] [G loss: 0.19]\n",
      "138 [D loss: 0.48, acc.: 73.83%] [Q loss: 2.14] [G loss: 0.22]\n",
      "139 [D loss: 0.53, acc.: 69.14%] [Q loss: 2.11] [G loss: 0.33]\n",
      "140 [D loss: 0.44, acc.: 78.12%] [Q loss: 2.07] [G loss: 0.18]\n",
      "141 [D loss: 0.55, acc.: 69.14%] [Q loss: 2.02] [G loss: 0.23]\n",
      "142 [D loss: 0.50, acc.: 71.88%] [Q loss: 2.03] [G loss: 0.20]\n",
      "143 [D loss: 0.53, acc.: 70.31%] [Q loss: 2.07] [G loss: 0.18]\n",
      "144 [D loss: 0.48, acc.: 74.22%] [Q loss: 1.98] [G loss: 0.21]\n",
      "145 [D loss: 0.53, acc.: 69.53%] [Q loss: 2.00] [G loss: 0.29]\n",
      "146 [D loss: 0.57, acc.: 69.92%] [Q loss: 2.09] [G loss: 0.19]\n",
      "147 [D loss: 0.56, acc.: 69.92%] [Q loss: 2.16] [G loss: 0.30]\n",
      "148 [D loss: 0.53, acc.: 70.31%] [Q loss: 2.02] [G loss: 0.11]\n",
      "149 [D loss: 0.60, acc.: 65.23%] [Q loss: 1.88] [G loss: 0.21]\n",
      "150 [D loss: 0.56, acc.: 70.31%] [Q loss: 1.93] [G loss: 0.23]\n",
      "151 [D loss: 0.49, acc.: 73.83%] [Q loss: 2.00] [G loss: 0.11]\n",
      "152 [D loss: 0.68, acc.: 62.89%] [Q loss: 2.08] [G loss: 0.21]\n",
      "153 [D loss: 0.50, acc.: 75.39%] [Q loss: 2.02] [G loss: 0.28]\n",
      "154 [D loss: 0.55, acc.: 67.58%] [Q loss: 1.76] [G loss: 0.26]\n",
      "155 [D loss: 0.63, acc.: 67.19%] [Q loss: 1.79] [G loss: 0.29]\n",
      "156 [D loss: 0.54, acc.: 67.58%] [Q loss: 2.00] [G loss: 0.21]\n",
      "157 [D loss: 0.57, acc.: 70.31%] [Q loss: 1.92] [G loss: 0.15]\n",
      "158 [D loss: 0.59, acc.: 66.02%] [Q loss: 1.97] [G loss: 0.22]\n",
      "159 [D loss: 0.58, acc.: 68.36%] [Q loss: 1.92] [G loss: 0.26]\n",
      "160 [D loss: 0.59, acc.: 67.19%] [Q loss: 1.86] [G loss: 0.16]\n",
      "161 [D loss: 0.53, acc.: 70.70%] [Q loss: 1.67] [G loss: 0.13]\n",
      "162 [D loss: 0.61, acc.: 61.33%] [Q loss: 1.68] [G loss: 0.14]\n",
      "163 [D loss: 0.55, acc.: 71.48%] [Q loss: 1.60] [G loss: 0.16]\n",
      "164 [D loss: 0.59, acc.: 65.23%] [Q loss: 1.52] [G loss: 0.23]\n",
      "165 [D loss: 0.68, acc.: 62.11%] [Q loss: 1.75] [G loss: 0.16]\n",
      "166 [D loss: 0.53, acc.: 71.48%] [Q loss: 1.77] [G loss: 0.21]\n",
      "167 [D loss: 0.57, acc.: 68.75%] [Q loss: 1.65] [G loss: 0.13]\n",
      "168 [D loss: 0.63, acc.: 67.58%] [Q loss: 1.60] [G loss: 0.10]\n",
      "169 [D loss: 0.61, acc.: 62.50%] [Q loss: 1.85] [G loss: 0.16]\n",
      "170 [D loss: 0.58, acc.: 65.23%] [Q loss: 1.65] [G loss: 0.21]\n",
      "171 [D loss: 0.62, acc.: 65.23%] [Q loss: 1.72] [G loss: 0.19]\n",
      "172 [D loss: 0.60, acc.: 64.84%] [Q loss: 1.86] [G loss: 0.15]\n",
      "173 [D loss: 0.65, acc.: 62.11%] [Q loss: 1.60] [G loss: 0.16]\n",
      "174 [D loss: 0.64, acc.: 64.06%] [Q loss: 1.50] [G loss: 0.21]\n",
      "175 [D loss: 0.65, acc.: 63.67%] [Q loss: 1.57] [G loss: 0.08]\n",
      "176 [D loss: 0.59, acc.: 67.19%] [Q loss: 1.69] [G loss: 0.14]\n",
      "177 [D loss: 0.57, acc.: 65.62%] [Q loss: 1.56] [G loss: 0.13]\n",
      "178 [D loss: 0.60, acc.: 64.06%] [Q loss: 1.70] [G loss: 0.11]\n",
      "179 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.58] [G loss: 0.19]\n",
      "180 [D loss: 0.63, acc.: 63.67%] [Q loss: 1.66] [G loss: 0.28]\n",
      "181 [D loss: 0.65, acc.: 64.06%] [Q loss: 1.54] [G loss: 0.08]\n",
      "182 [D loss: 0.56, acc.: 66.41%] [Q loss: 1.73] [G loss: 0.18]\n",
      "183 [D loss: 0.56, acc.: 69.53%] [Q loss: 1.70] [G loss: 0.18]\n",
      "184 [D loss: 0.77, acc.: 56.25%] [Q loss: 1.50] [G loss: 0.15]\n",
      "185 [D loss: 0.77, acc.: 59.77%] [Q loss: 1.74] [G loss: 0.26]\n",
      "186 [D loss: 0.59, acc.: 68.75%] [Q loss: 1.61] [G loss: 0.19]\n",
      "187 [D loss: 0.70, acc.: 64.06%] [Q loss: 1.36] [G loss: 0.10]\n",
      "188 [D loss: 0.65, acc.: 62.89%] [Q loss: 1.52] [G loss: 0.12]\n",
      "189 [D loss: 0.52, acc.: 74.61%] [Q loss: 1.66] [G loss: 0.08]\n",
      "190 [D loss: 0.65, acc.: 63.28%] [Q loss: 1.61] [G loss: 0.09]\n",
      "191 [D loss: 0.74, acc.: 58.20%] [Q loss: 1.63] [G loss: 0.12]\n",
      "192 [D loss: 0.70, acc.: 59.77%] [Q loss: 1.71] [G loss: 0.20]\n",
      "193 [D loss: 0.59, acc.: 71.88%] [Q loss: 1.52] [G loss: 0.15]\n",
      "194 [D loss: 0.65, acc.: 66.41%] [Q loss: 1.34] [G loss: 0.07]\n",
      "195 [D loss: 0.62, acc.: 62.50%] [Q loss: 1.55] [G loss: 0.14]\n",
      "196 [D loss: 0.62, acc.: 65.62%] [Q loss: 1.53] [G loss: 0.09]\n",
      "197 [D loss: 0.69, acc.: 58.98%] [Q loss: 1.39] [G loss: 0.10]\n",
      "198 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.49] [G loss: 0.13]\n",
      "199 [D loss: 0.64, acc.: 67.19%] [Q loss: 1.59] [G loss: 0.14]\n",
      "200 [D loss: 0.68, acc.: 62.11%] [Q loss: 1.44] [G loss: 0.06]\n",
      "201 [D loss: 0.60, acc.: 66.41%] [Q loss: 1.55] [G loss: 0.08]\n",
      "202 [D loss: 0.64, acc.: 63.28%] [Q loss: 1.53] [G loss: 0.08]\n",
      "203 [D loss: 0.64, acc.: 62.50%] [Q loss: 1.44] [G loss: 0.11]\n",
      "204 [D loss: 0.62, acc.: 62.11%] [Q loss: 1.55] [G loss: 0.13]\n",
      "205 [D loss: 0.71, acc.: 61.33%] [Q loss: 1.40] [G loss: 0.06]\n",
      "206 [D loss: 0.70, acc.: 59.38%] [Q loss: 1.38] [G loss: 0.11]\n",
      "207 [D loss: 0.70, acc.: 59.77%] [Q loss: 1.39] [G loss: 0.13]\n",
      "208 [D loss: 0.66, acc.: 62.50%] [Q loss: 1.45] [G loss: 0.08]\n",
      "209 [D loss: 0.69, acc.: 58.59%] [Q loss: 1.32] [G loss: 0.11]\n",
      "210 [D loss: 0.67, acc.: 61.33%] [Q loss: 1.30] [G loss: 0.09]\n",
      "211 [D loss: 0.62, acc.: 64.06%] [Q loss: 1.43] [G loss: 0.09]\n",
      "212 [D loss: 0.68, acc.: 60.16%] [Q loss: 1.28] [G loss: 0.12]\n",
      "213 [D loss: 0.72, acc.: 57.42%] [Q loss: 1.38] [G loss: 0.11]\n",
      "214 [D loss: 0.74, acc.: 57.42%] [Q loss: 1.35] [G loss: 0.07]\n",
      "215 [D loss: 0.66, acc.: 65.23%] [Q loss: 1.39] [G loss: 0.10]\n",
      "216 [D loss: 0.64, acc.: 65.23%] [Q loss: 1.39] [G loss: 0.08]\n",
      "217 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.25] [G loss: 0.11]\n",
      "218 [D loss: 0.64, acc.: 63.28%] [Q loss: 1.38] [G loss: 0.05]\n",
      "219 [D loss: 0.68, acc.: 61.33%] [Q loss: 1.27] [G loss: 0.09]\n",
      "220 [D loss: 0.68, acc.: 61.33%] [Q loss: 1.32] [G loss: 0.09]\n",
      "221 [D loss: 0.70, acc.: 58.59%] [Q loss: 1.25] [G loss: 0.06]\n",
      "222 [D loss: 0.75, acc.: 54.69%] [Q loss: 1.25] [G loss: 0.08]\n",
      "223 [D loss: 0.74, acc.: 55.86%] [Q loss: 1.32] [G loss: 0.07]\n",
      "224 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.33] [G loss: 0.16]\n",
      "225 [D loss: 0.71, acc.: 63.67%] [Q loss: 1.27] [G loss: 0.09]\n",
      "226 [D loss: 0.71, acc.: 58.20%] [Q loss: 1.33] [G loss: 0.08]\n",
      "227 [D loss: 0.62, acc.: 64.06%] [Q loss: 1.33] [G loss: 0.07]\n",
      "228 [D loss: 0.65, acc.: 60.16%] [Q loss: 1.24] [G loss: 0.05]\n",
      "229 [D loss: 0.68, acc.: 62.50%] [Q loss: 1.30] [G loss: 0.09]\n",
      "230 [D loss: 0.63, acc.: 65.23%] [Q loss: 1.41] [G loss: 0.06]\n",
      "231 [D loss: 0.79, acc.: 53.52%] [Q loss: 1.31] [G loss: 0.08]\n",
      "232 [D loss: 0.69, acc.: 60.16%] [Q loss: 1.33] [G loss: 0.12]\n",
      "233 [D loss: 0.70, acc.: 60.16%] [Q loss: 1.24] [G loss: 0.17]\n",
      "234 [D loss: 0.73, acc.: 61.33%] [Q loss: 1.22] [G loss: 0.05]\n",
      "235 [D loss: 0.71, acc.: 61.72%] [Q loss: 1.24] [G loss: 0.06]\n",
      "236 [D loss: 0.70, acc.: 59.77%] [Q loss: 1.22] [G loss: 0.12]\n",
      "237 [D loss: 0.73, acc.: 57.42%] [Q loss: 1.27] [G loss: 0.11]\n",
      "238 [D loss: 0.66, acc.: 64.84%] [Q loss: 1.29] [G loss: 0.09]\n",
      "239 [D loss: 0.71, acc.: 55.08%] [Q loss: 1.18] [G loss: 0.09]\n",
      "240 [D loss: 0.67, acc.: 61.72%] [Q loss: 1.34] [G loss: 0.13]\n",
      "241 [D loss: 0.72, acc.: 58.98%] [Q loss: 1.21] [G loss: 0.09]\n",
      "242 [D loss: 0.65, acc.: 62.11%] [Q loss: 1.29] [G loss: 0.11]\n",
      "243 [D loss: 0.64, acc.: 64.84%] [Q loss: 1.19] [G loss: 0.09]\n",
      "244 [D loss: 0.68, acc.: 59.38%] [Q loss: 1.28] [G loss: 0.03]\n",
      "245 [D loss: 0.66, acc.: 61.72%] [Q loss: 1.20] [G loss: 0.08]\n",
      "246 [D loss: 0.69, acc.: 60.55%] [Q loss: 1.43] [G loss: 0.05]\n",
      "247 [D loss: 0.62, acc.: 62.11%] [Q loss: 1.33] [G loss: 0.06]\n",
      "248 [D loss: 0.66, acc.: 60.94%] [Q loss: 1.36] [G loss: 0.07]\n",
      "249 [D loss: 0.77, acc.: 53.52%] [Q loss: 1.23] [G loss: 0.10]\n",
      "250 [D loss: 0.71, acc.: 57.03%] [Q loss: 1.24] [G loss: 0.14]\n",
      "251 [D loss: 0.68, acc.: 61.72%] [Q loss: 1.28] [G loss: 0.06]\n",
      "252 [D loss: 0.69, acc.: 59.38%] [Q loss: 1.24] [G loss: 0.06]\n",
      "253 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.24] [G loss: 0.14]\n",
      "254 [D loss: 0.70, acc.: 58.59%] [Q loss: 1.19] [G loss: 0.04]\n",
      "255 [D loss: 0.77, acc.: 58.98%] [Q loss: 1.26] [G loss: 0.05]\n",
      "256 [D loss: 0.66, acc.: 61.33%] [Q loss: 1.31] [G loss: 0.05]\n",
      "257 [D loss: 0.70, acc.: 58.20%] [Q loss: 1.22] [G loss: 0.07]\n",
      "258 [D loss: 0.72, acc.: 59.77%] [Q loss: 1.25] [G loss: 0.11]\n",
      "259 [D loss: 0.70, acc.: 60.94%] [Q loss: 1.26] [G loss: 0.11]\n",
      "260 [D loss: 0.75, acc.: 58.98%] [Q loss: 1.27] [G loss: 0.09]\n",
      "261 [D loss: 0.67, acc.: 63.67%] [Q loss: 1.26] [G loss: 0.10]\n",
      "262 [D loss: 0.70, acc.: 58.98%] [Q loss: 1.21] [G loss: 0.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263 [D loss: 0.69, acc.: 59.77%] [Q loss: 1.22] [G loss: 0.07]\n",
      "264 [D loss: 0.72, acc.: 58.98%] [Q loss: 1.19] [G loss: 0.06]\n",
      "265 [D loss: 0.69, acc.: 60.16%] [Q loss: 1.28] [G loss: 0.06]\n",
      "266 [D loss: 0.73, acc.: 56.64%] [Q loss: 1.17] [G loss: 0.13]\n",
      "267 [D loss: 0.63, acc.: 64.84%] [Q loss: 1.26] [G loss: 0.11]\n",
      "268 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.15] [G loss: 0.05]\n",
      "269 [D loss: 0.67, acc.: 62.89%] [Q loss: 1.16] [G loss: 0.13]\n",
      "270 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.15] [G loss: 0.06]\n",
      "271 [D loss: 0.75, acc.: 57.03%] [Q loss: 1.16] [G loss: 0.09]\n",
      "272 [D loss: 0.67, acc.: 61.72%] [Q loss: 1.18] [G loss: 0.05]\n",
      "273 [D loss: 0.73, acc.: 60.16%] [Q loss: 1.15] [G loss: 0.10]\n",
      "274 [D loss: 0.76, acc.: 54.69%] [Q loss: 1.16] [G loss: 0.11]\n",
      "275 [D loss: 0.69, acc.: 60.55%] [Q loss: 1.16] [G loss: 0.12]\n",
      "276 [D loss: 0.74, acc.: 57.81%] [Q loss: 1.24] [G loss: 0.05]\n",
      "277 [D loss: 0.74, acc.: 59.38%] [Q loss: 1.21] [G loss: 0.08]\n",
      "278 [D loss: 0.66, acc.: 60.94%] [Q loss: 1.15] [G loss: 0.10]\n",
      "279 [D loss: 0.67, acc.: 58.20%] [Q loss: 1.16] [G loss: 0.04]\n",
      "280 [D loss: 0.68, acc.: 57.03%] [Q loss: 1.15] [G loss: 0.06]\n",
      "281 [D loss: 0.67, acc.: 60.55%] [Q loss: 1.19] [G loss: 0.10]\n",
      "282 [D loss: 0.75, acc.: 60.55%] [Q loss: 1.17] [G loss: 0.11]\n",
      "283 [D loss: 0.69, acc.: 58.59%] [Q loss: 1.22] [G loss: 0.10]\n",
      "284 [D loss: 0.66, acc.: 60.55%] [Q loss: 1.23] [G loss: 0.09]\n",
      "285 [D loss: 0.68, acc.: 62.50%] [Q loss: 1.14] [G loss: 0.05]\n",
      "286 [D loss: 0.72, acc.: 57.81%] [Q loss: 1.16] [G loss: 0.09]\n",
      "287 [D loss: 0.70, acc.: 58.59%] [Q loss: 1.08] [G loss: 0.09]\n",
      "288 [D loss: 0.71, acc.: 59.77%] [Q loss: 1.05] [G loss: 0.11]\n",
      "289 [D loss: 0.68, acc.: 59.38%] [Q loss: 1.16] [G loss: 0.07]\n",
      "290 [D loss: 0.69, acc.: 59.77%] [Q loss: 1.15] [G loss: 0.14]\n",
      "291 [D loss: 0.70, acc.: 57.42%] [Q loss: 1.12] [G loss: 0.15]\n",
      "292 [D loss: 0.66, acc.: 62.50%] [Q loss: 1.16] [G loss: 0.08]\n",
      "293 [D loss: 0.66, acc.: 60.55%] [Q loss: 1.14] [G loss: 0.03]\n",
      "294 [D loss: 0.68, acc.: 61.33%] [Q loss: 1.04] [G loss: 0.09]\n",
      "295 [D loss: 0.68, acc.: 58.98%] [Q loss: 1.09] [G loss: 0.09]\n",
      "296 [D loss: 0.64, acc.: 62.11%] [Q loss: 1.12] [G loss: 0.06]\n",
      "297 [D loss: 0.66, acc.: 61.72%] [Q loss: 1.11] [G loss: 0.03]\n",
      "298 [D loss: 0.66, acc.: 58.98%] [Q loss: 1.12] [G loss: 0.07]\n",
      "299 [D loss: 0.72, acc.: 55.47%] [Q loss: 1.09] [G loss: 0.15]\n",
      "300 [D loss: 0.68, acc.: 62.11%] [Q loss: 1.11] [G loss: 0.06]\n",
      "301 [D loss: 0.72, acc.: 58.98%] [Q loss: 1.09] [G loss: 0.07]\n",
      "302 [D loss: 0.73, acc.: 56.25%] [Q loss: 1.04] [G loss: 0.05]\n",
      "303 [D loss: 0.71, acc.: 61.72%] [Q loss: 1.16] [G loss: 0.06]\n",
      "304 [D loss: 0.62, acc.: 65.62%] [Q loss: 1.14] [G loss: 0.11]\n",
      "305 [D loss: 0.71, acc.: 57.81%] [Q loss: 1.10] [G loss: 0.08]\n",
      "306 [D loss: 0.69, acc.: 62.50%] [Q loss: 1.05] [G loss: 0.04]\n",
      "307 [D loss: 0.73, acc.: 60.16%] [Q loss: 1.10] [G loss: 0.10]\n",
      "308 [D loss: 0.66, acc.: 62.50%] [Q loss: 1.07] [G loss: 0.07]\n",
      "309 [D loss: 0.71, acc.: 60.16%] [Q loss: 1.12] [G loss: 0.10]\n",
      "310 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.14] [G loss: 0.08]\n",
      "311 [D loss: 0.68, acc.: 60.94%] [Q loss: 1.10] [G loss: 0.08]\n",
      "312 [D loss: 0.73, acc.: 57.03%] [Q loss: 1.15] [G loss: 0.06]\n",
      "313 [D loss: 0.73, acc.: 57.03%] [Q loss: 1.12] [G loss: 0.09]\n",
      "314 [D loss: 0.67, acc.: 60.16%] [Q loss: 1.09] [G loss: 0.05]\n",
      "315 [D loss: 0.68, acc.: 58.59%] [Q loss: 1.18] [G loss: 0.08]\n",
      "316 [D loss: 0.67, acc.: 60.94%] [Q loss: 1.05] [G loss: 0.08]\n",
      "317 [D loss: 0.66, acc.: 60.55%] [Q loss: 1.07] [G loss: 0.06]\n",
      "318 [D loss: 0.71, acc.: 57.42%] [Q loss: 1.07] [G loss: 0.03]\n",
      "319 [D loss: 0.69, acc.: 61.33%] [Q loss: 1.05] [G loss: 0.06]\n",
      "320 [D loss: 0.63, acc.: 60.94%] [Q loss: 1.05] [G loss: 0.06]\n",
      "321 [D loss: 0.71, acc.: 58.20%] [Q loss: 1.16] [G loss: 0.05]\n",
      "322 [D loss: 0.67, acc.: 57.03%] [Q loss: 1.03] [G loss: 0.08]\n",
      "323 [D loss: 0.71, acc.: 56.64%] [Q loss: 1.13] [G loss: 0.05]\n",
      "324 [D loss: 0.69, acc.: 59.77%] [Q loss: 1.10] [G loss: 0.07]\n",
      "325 [D loss: 0.69, acc.: 59.77%] [Q loss: 1.08] [G loss: 0.04]\n",
      "326 [D loss: 0.73, acc.: 57.81%] [Q loss: 1.09] [G loss: 0.04]\n",
      "327 [D loss: 0.68, acc.: 60.94%] [Q loss: 1.14] [G loss: 0.10]\n",
      "328 [D loss: 0.73, acc.: 54.30%] [Q loss: 1.16] [G loss: 0.06]\n",
      "329 [D loss: 0.68, acc.: 56.64%] [Q loss: 1.13] [G loss: 0.05]\n",
      "330 [D loss: 0.67, acc.: 62.89%] [Q loss: 1.10] [G loss: 0.03]\n",
      "331 [D loss: 0.71, acc.: 61.33%] [Q loss: 1.07] [G loss: 0.07]\n",
      "332 [D loss: 0.65, acc.: 59.77%] [Q loss: 1.00] [G loss: 0.07]\n",
      "333 [D loss: 0.70, acc.: 58.20%] [Q loss: 1.09] [G loss: 0.04]\n",
      "334 [D loss: 0.78, acc.: 55.08%] [Q loss: 1.05] [G loss: 0.04]\n",
      "335 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.10] [G loss: 0.04]\n",
      "336 [D loss: 0.75, acc.: 57.03%] [Q loss: 1.21] [G loss: 0.10]\n",
      "337 [D loss: 0.71, acc.: 59.38%] [Q loss: 1.11] [G loss: 0.13]\n",
      "338 [D loss: 0.67, acc.: 62.50%] [Q loss: 1.10] [G loss: 0.04]\n",
      "339 [D loss: 0.73, acc.: 61.33%] [Q loss: 1.09] [G loss: 0.08]\n",
      "340 [D loss: 0.71, acc.: 58.98%] [Q loss: 1.01] [G loss: 0.05]\n",
      "341 [D loss: 0.68, acc.: 60.94%] [Q loss: 1.04] [G loss: 0.03]\n",
      "342 [D loss: 0.65, acc.: 63.67%] [Q loss: 1.10] [G loss: 0.03]\n",
      "343 [D loss: 0.66, acc.: 61.72%] [Q loss: 1.07] [G loss: 0.02]\n",
      "344 [D loss: 0.66, acc.: 61.72%] [Q loss: 1.15] [G loss: 0.04]\n",
      "345 [D loss: 0.72, acc.: 57.81%] [Q loss: 1.02] [G loss: 0.04]\n",
      "346 [D loss: 0.68, acc.: 57.03%] [Q loss: 1.01] [G loss: 0.02]\n",
      "347 [D loss: 0.72, acc.: 60.16%] [Q loss: 1.08] [G loss: 0.04]\n",
      "348 [D loss: 0.70, acc.: 58.59%] [Q loss: 1.11] [G loss: 0.10]\n",
      "349 [D loss: 0.68, acc.: 62.11%] [Q loss: 1.08] [G loss: 0.07]\n",
      "350 [D loss: 0.70, acc.: 60.55%] [Q loss: 0.95] [G loss: 0.07]\n",
      "351 [D loss: 0.72, acc.: 58.20%] [Q loss: 0.95] [G loss: 0.05]\n",
      "352 [D loss: 0.67, acc.: 61.33%] [Q loss: 1.08] [G loss: 0.04]\n",
      "353 [D loss: 0.65, acc.: 60.16%] [Q loss: 1.06] [G loss: 0.02]\n",
      "354 [D loss: 0.67, acc.: 61.72%] [Q loss: 1.07] [G loss: 0.03]\n",
      "355 [D loss: 0.62, acc.: 66.02%] [Q loss: 1.05] [G loss: 0.06]\n",
      "356 [D loss: 0.72, acc.: 56.25%] [Q loss: 1.06] [G loss: 0.01]\n",
      "357 [D loss: 0.67, acc.: 59.77%] [Q loss: 1.07] [G loss: 0.07]\n",
      "358 [D loss: 0.76, acc.: 55.47%] [Q loss: 1.07] [G loss: 0.07]\n",
      "359 [D loss: 0.70, acc.: 57.81%] [Q loss: 1.04] [G loss: 0.04]\n",
      "360 [D loss: 0.74, acc.: 56.64%] [Q loss: 1.06] [G loss: 0.06]\n",
      "361 [D loss: 0.70, acc.: 60.16%] [Q loss: 1.10] [G loss: 0.13]\n",
      "362 [D loss: 0.65, acc.: 66.41%] [Q loss: 1.05] [G loss: 0.04]\n",
      "363 [D loss: 0.68, acc.: 60.16%] [Q loss: 1.04] [G loss: 0.08]\n",
      "364 [D loss: 0.69, acc.: 58.20%] [Q loss: 1.04] [G loss: 0.07]\n",
      "365 [D loss: 0.68, acc.: 60.94%] [Q loss: 1.10] [G loss: 0.09]\n",
      "366 [D loss: 0.70, acc.: 58.20%] [Q loss: 1.10] [G loss: 0.06]\n",
      "367 [D loss: 0.68, acc.: 58.59%] [Q loss: 1.07] [G loss: 0.08]\n",
      "368 [D loss: 0.68, acc.: 61.33%] [Q loss: 0.98] [G loss: 0.03]\n",
      "369 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.07] [G loss: 0.03]\n",
      "370 [D loss: 0.72, acc.: 58.59%] [Q loss: 1.06] [G loss: 0.06]\n",
      "371 [D loss: 0.66, acc.: 61.72%] [Q loss: 1.06] [G loss: 0.05]\n",
      "372 [D loss: 0.67, acc.: 62.89%] [Q loss: 1.10] [G loss: 0.15]\n",
      "373 [D loss: 0.66, acc.: 62.89%] [Q loss: 1.07] [G loss: 0.03]\n",
      "374 [D loss: 0.72, acc.: 57.03%] [Q loss: 1.00] [G loss: 0.07]\n",
      "375 [D loss: 0.69, acc.: 61.33%] [Q loss: 1.06] [G loss: 0.08]\n",
      "376 [D loss: 0.67, acc.: 60.55%] [Q loss: 1.13] [G loss: 0.10]\n",
      "377 [D loss: 0.71, acc.: 58.59%] [Q loss: 1.08] [G loss: 0.10]\n",
      "378 [D loss: 0.64, acc.: 60.16%] [Q loss: 1.02] [G loss: 0.07]\n",
      "379 [D loss: 0.71, acc.: 60.16%] [Q loss: 1.13] [G loss: 0.05]\n",
      "380 [D loss: 0.63, acc.: 60.55%] [Q loss: 1.02] [G loss: 0.03]\n",
      "381 [D loss: 0.73, acc.: 56.25%] [Q loss: 1.07] [G loss: 0.10]\n",
      "382 [D loss: 0.68, acc.: 58.59%] [Q loss: 1.04] [G loss: 0.03]\n",
      "383 [D loss: 0.69, acc.: 61.33%] [Q loss: 1.07] [G loss: 0.05]\n",
      "384 [D loss: 0.70, acc.: 57.42%] [Q loss: 1.07] [G loss: 0.08]\n",
      "385 [D loss: 0.68, acc.: 61.33%] [Q loss: 1.00] [G loss: 0.06]\n",
      "386 [D loss: 0.64, acc.: 64.45%] [Q loss: 1.00] [G loss: 0.07]\n",
      "387 [D loss: 0.69, acc.: 58.59%] [Q loss: 1.02] [G loss: 0.05]\n",
      "388 [D loss: 0.71, acc.: 57.81%] [Q loss: 1.01] [G loss: 0.08]\n",
      "389 [D loss: 0.64, acc.: 62.50%] [Q loss: 1.03] [G loss: 0.03]\n",
      "390 [D loss: 0.70, acc.: 60.55%] [Q loss: 1.05] [G loss: 0.05]\n",
      "391 [D loss: 0.67, acc.: 59.38%] [Q loss: 1.03] [G loss: 0.06]\n",
      "392 [D loss: 0.69, acc.: 61.72%] [Q loss: 1.05] [G loss: 0.06]\n",
      "393 [D loss: 0.68, acc.: 60.16%] [Q loss: 1.00] [G loss: 0.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.66, acc.: 60.94%] [Q loss: 1.03] [G loss: 0.12]\n",
      "395 [D loss: 0.64, acc.: 61.72%] [Q loss: 1.07] [G loss: 0.06]\n",
      "396 [D loss: 0.75, acc.: 58.98%] [Q loss: 1.03] [G loss: 0.04]\n",
      "397 [D loss: 0.72, acc.: 58.59%] [Q loss: 0.91] [G loss: 0.08]\n",
      "398 [D loss: 0.67, acc.: 57.03%] [Q loss: 1.05] [G loss: 0.03]\n",
      "399 [D loss: 0.66, acc.: 58.59%] [Q loss: 1.07] [G loss: 0.03]\n",
      "400 [D loss: 0.65, acc.: 62.89%] [Q loss: 1.08] [G loss: 0.02]\n",
      "401 [D loss: 0.66, acc.: 58.59%] [Q loss: 1.07] [G loss: 0.08]\n",
      "402 [D loss: 0.65, acc.: 61.33%] [Q loss: 1.07] [G loss: 0.03]\n",
      "403 [D loss: 0.71, acc.: 55.47%] [Q loss: 1.12] [G loss: 0.04]\n",
      "404 [D loss: 0.68, acc.: 60.55%] [Q loss: 1.04] [G loss: 0.06]\n",
      "405 [D loss: 0.69, acc.: 61.33%] [Q loss: 1.02] [G loss: 0.05]\n",
      "406 [D loss: 0.71, acc.: 57.03%] [Q loss: 1.02] [G loss: 0.06]\n",
      "407 [D loss: 0.74, acc.: 60.16%] [Q loss: 0.97] [G loss: 0.09]\n",
      "408 [D loss: 0.76, acc.: 58.20%] [Q loss: 0.99] [G loss: 0.09]\n",
      "409 [D loss: 0.72, acc.: 57.03%] [Q loss: 1.04] [G loss: 0.10]\n",
      "410 [D loss: 0.71, acc.: 59.38%] [Q loss: 1.14] [G loss: 0.06]\n",
      "411 [D loss: 0.66, acc.: 62.11%] [Q loss: 0.92] [G loss: 0.04]\n",
      "412 [D loss: 0.71, acc.: 59.38%] [Q loss: 0.99] [G loss: 0.05]\n",
      "413 [D loss: 0.75, acc.: 55.86%] [Q loss: 0.97] [G loss: 0.04]\n",
      "414 [D loss: 0.68, acc.: 56.64%] [Q loss: 0.96] [G loss: 0.05]\n",
      "415 [D loss: 0.71, acc.: 57.42%] [Q loss: 0.94] [G loss: 0.08]\n",
      "416 [D loss: 0.69, acc.: 60.55%] [Q loss: 1.10] [G loss: 0.04]\n",
      "417 [D loss: 0.70, acc.: 60.16%] [Q loss: 1.03] [G loss: 0.03]\n",
      "418 [D loss: 0.66, acc.: 60.16%] [Q loss: 1.08] [G loss: 0.04]\n",
      "419 [D loss: 0.64, acc.: 64.84%] [Q loss: 0.96] [G loss: 0.09]\n",
      "420 [D loss: 0.69, acc.: 58.59%] [Q loss: 0.98] [G loss: 0.05]\n",
      "421 [D loss: 0.70, acc.: 62.11%] [Q loss: 0.97] [G loss: 0.09]\n",
      "422 [D loss: 0.69, acc.: 58.98%] [Q loss: 1.01] [G loss: 0.08]\n",
      "423 [D loss: 0.67, acc.: 58.59%] [Q loss: 0.96] [G loss: 0.05]\n",
      "424 [D loss: 0.69, acc.: 61.72%] [Q loss: 0.96] [G loss: 0.06]\n",
      "425 [D loss: 0.69, acc.: 62.11%] [Q loss: 0.99] [G loss: 0.08]\n",
      "426 [D loss: 0.70, acc.: 60.16%] [Q loss: 0.92] [G loss: 0.13]\n",
      "427 [D loss: 0.67, acc.: 60.55%] [Q loss: 1.07] [G loss: 0.06]\n",
      "428 [D loss: 0.69, acc.: 59.38%] [Q loss: 1.01] [G loss: 0.07]\n",
      "429 [D loss: 0.71, acc.: 54.69%] [Q loss: 1.06] [G loss: 0.09]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-88917d01b351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0minfogan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINFOGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m     \u001b[0minfogan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-88917d01b351>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# ---------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class INFOGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.num_classes = 10\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 72\n",
    "\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        losses = ['binary_crossentropy', self.mutual_info_loss]\n",
    "\n",
    "        # Build and the discriminator and recognition network\n",
    "        self.discriminator, self.auxilliary = self.build_disk_and_q_net()\n",
    "\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the recognition network Q\n",
    "        self.auxilliary.compile(loss=[self.mutual_info_loss],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        gen_input = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(gen_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # The recognition network produces the label\n",
    "        target_label = self.auxilliary(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        self.combined = Model(gen_input, [valid, target_label])\n",
    "        self.combined.compile(loss=losses,\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        gen_input = Input(shape=(self.latent_dim,))\n",
    "        img = model(gen_input)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(gen_input, img)\n",
    "\n",
    "\n",
    "    def build_disk_and_q_net(self):\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Shared layers between discriminator and recognition network\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        img_embedding = model(img)\n",
    "\n",
    "        # Discriminator\n",
    "        validity = Dense(1, activation='sigmoid')(img_embedding)\n",
    "\n",
    "        # Recognition\n",
    "        q_net = Dense(128, activation='relu')(img_embedding)\n",
    "        label = Dense(self.num_classes, activation='softmax')(q_net)\n",
    "\n",
    "        # Return discriminator and recognition network\n",
    "        return Model(img, validity), Model(img, label)\n",
    "\n",
    "\n",
    "    def mutual_info_loss(self, c, c_given_x):\n",
    "        \"\"\"The mutual information metric we aim to minimize\"\"\"\n",
    "        eps = 1e-8\n",
    "        conditional_entropy = K.mean(- K.sum(K.log(c_given_x + eps) * c, axis=1))\n",
    "        entropy = K.mean(- K.sum(K.log(c + eps) * c, axis=1))\n",
    "\n",
    "        return conditional_entropy + entropy\n",
    "\n",
    "    def sample_generator_input(self, batch_size):\n",
    "        # Generator inputs\n",
    "        sampled_noise = np.random.normal(0, 1, (batch_size, 62))\n",
    "        sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "        sampled_labels = to_categorical(sampled_labels, num_classes=self.num_classes)\n",
    "\n",
    "        return sampled_noise, sampled_labels\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and categorical labels\n",
    "            sampled_noise, sampled_labels = self.sample_generator_input(batch_size)\n",
    "            gen_input = np.concatenate((sampled_noise, sampled_labels), axis=1)\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(gen_input)\n",
    "\n",
    "            # Train on real and generated data\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "            # Avg. loss\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator and Q-network\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(gen_input, [valid, sampled_labels])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %.2f, acc.: %.2f%%] [Q loss: %.2f] [G loss: %.2f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[1], g_loss[2]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 10, 10\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for i in range(c):\n",
    "            sampled_noise, _ = self.sample_generator_input(c)\n",
    "            label = to_categorical(np.full(fill_value=i, shape=(r,1)), num_classes=self.num_classes)\n",
    "            gen_input = np.concatenate((sampled_noise, label), axis=1)\n",
    "            gen_imgs = self.generator.predict(gen_input)\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            for j in range(r):\n",
    "                axs[j,i].imshow(gen_imgs[j,:,:,0], cmap='gray')\n",
    "                axs[j,i].axis('off')\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    infogan = INFOGAN()\n",
    "    infogan.train(epochs=50000, batch_size=128, sample_interval=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
