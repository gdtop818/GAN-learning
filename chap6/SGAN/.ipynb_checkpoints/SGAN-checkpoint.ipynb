{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 388,608\n",
      "Trainable params: 388,224\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adward\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.579261, acc: 39.06%, op_acc: 7.81%] [G loss: 0.713937]\n",
      "1 [D loss: 0.416571, acc: 70.31%, op_acc: 7.81%] [G loss: 0.773397]\n",
      "2 [D loss: 0.428133, acc: 64.06%, op_acc: 7.81%] [G loss: 0.821273]\n",
      "3 [D loss: 0.366195, acc: 75.00%, op_acc: 17.19%] [G loss: 1.253886]\n",
      "4 [D loss: 0.316966, acc: 81.25%, op_acc: 14.06%] [G loss: 1.117550]\n",
      "5 [D loss: 0.299595, acc: 89.06%, op_acc: 12.50%] [G loss: 1.096185]\n",
      "6 [D loss: 0.261588, acc: 87.50%, op_acc: 15.62%] [G loss: 1.531133]\n",
      "7 [D loss: 0.255547, acc: 92.19%, op_acc: 17.19%] [G loss: 1.341451]\n",
      "8 [D loss: 0.241427, acc: 93.75%, op_acc: 7.81%] [G loss: 1.822409]\n",
      "9 [D loss: 0.251653, acc: 90.62%, op_acc: 7.81%] [G loss: 1.598400]\n",
      "10 [D loss: 0.260286, acc: 87.50%, op_acc: 9.38%] [G loss: 1.976312]\n",
      "11 [D loss: 0.247217, acc: 92.19%, op_acc: 18.75%] [G loss: 2.247524]\n",
      "12 [D loss: 0.240856, acc: 92.19%, op_acc: 20.31%] [G loss: 2.127833]\n",
      "13 [D loss: 0.223165, acc: 98.44%, op_acc: 20.31%] [G loss: 2.329797]\n",
      "14 [D loss: 0.200274, acc: 96.88%, op_acc: 29.69%] [G loss: 2.187803]\n",
      "15 [D loss: 0.213020, acc: 93.75%, op_acc: 23.44%] [G loss: 2.028457]\n",
      "16 [D loss: 0.178231, acc: 98.44%, op_acc: 34.38%] [G loss: 2.419028]\n",
      "17 [D loss: 0.182228, acc: 96.88%, op_acc: 18.75%] [G loss: 2.722236]\n",
      "18 [D loss: 0.189583, acc: 98.44%, op_acc: 23.44%] [G loss: 2.298924]\n",
      "19 [D loss: 0.182194, acc: 96.88%, op_acc: 34.38%] [G loss: 2.762452]\n",
      "20 [D loss: 0.156544, acc: 98.44%, op_acc: 29.69%] [G loss: 2.940851]\n",
      "21 [D loss: 0.148608, acc: 100.00%, op_acc: 21.88%] [G loss: 3.637377]\n",
      "22 [D loss: 0.132628, acc: 100.00%, op_acc: 37.50%] [G loss: 3.538563]\n",
      "23 [D loss: 0.126248, acc: 100.00%, op_acc: 32.81%] [G loss: 3.357234]\n",
      "24 [D loss: 0.118457, acc: 98.44%, op_acc: 40.62%] [G loss: 3.185118]\n",
      "25 [D loss: 0.135417, acc: 100.00%, op_acc: 46.88%] [G loss: 3.321160]\n",
      "26 [D loss: 0.123380, acc: 98.44%, op_acc: 48.44%] [G loss: 3.420177]\n",
      "27 [D loss: 0.102543, acc: 100.00%, op_acc: 46.88%] [G loss: 3.273412]\n",
      "28 [D loss: 0.112372, acc: 100.00%, op_acc: 54.69%] [G loss: 3.419446]\n",
      "29 [D loss: 0.095310, acc: 100.00%, op_acc: 56.25%] [G loss: 3.424036]\n",
      "30 [D loss: 0.109416, acc: 100.00%, op_acc: 57.81%] [G loss: 3.554301]\n",
      "31 [D loss: 0.108390, acc: 98.44%, op_acc: 56.25%] [G loss: 3.610052]\n",
      "32 [D loss: 0.097119, acc: 98.44%, op_acc: 57.81%] [G loss: 3.489865]\n",
      "33 [D loss: 0.122958, acc: 96.88%, op_acc: 56.25%] [G loss: 2.902096]\n",
      "34 [D loss: 0.109082, acc: 100.00%, op_acc: 43.75%] [G loss: 3.320475]\n",
      "35 [D loss: 0.119301, acc: 98.44%, op_acc: 53.12%] [G loss: 3.057492]\n",
      "36 [D loss: 0.092512, acc: 100.00%, op_acc: 46.88%] [G loss: 3.738100]\n",
      "37 [D loss: 0.090150, acc: 100.00%, op_acc: 57.81%] [G loss: 3.383219]\n",
      "38 [D loss: 0.098135, acc: 100.00%, op_acc: 54.69%] [G loss: 3.323831]\n",
      "39 [D loss: 0.091537, acc: 100.00%, op_acc: 71.88%] [G loss: 3.011496]\n",
      "40 [D loss: 0.120732, acc: 96.88%, op_acc: 45.31%] [G loss: 3.451717]\n",
      "41 [D loss: 0.105684, acc: 100.00%, op_acc: 59.38%] [G loss: 4.461803]\n",
      "42 [D loss: 0.099341, acc: 98.44%, op_acc: 65.62%] [G loss: 3.419014]\n",
      "43 [D loss: 0.087519, acc: 100.00%, op_acc: 62.50%] [G loss: 3.581265]\n",
      "44 [D loss: 0.084844, acc: 100.00%, op_acc: 67.19%] [G loss: 3.178320]\n",
      "45 [D loss: 0.098498, acc: 100.00%, op_acc: 57.81%] [G loss: 3.072642]\n",
      "46 [D loss: 0.088724, acc: 100.00%, op_acc: 65.62%] [G loss: 2.836571]\n",
      "47 [D loss: 0.087728, acc: 100.00%, op_acc: 60.94%] [G loss: 3.225039]\n",
      "48 [D loss: 0.077352, acc: 100.00%, op_acc: 65.62%] [G loss: 3.543608]\n",
      "49 [D loss: 0.085863, acc: 100.00%, op_acc: 70.31%] [G loss: 3.064190]\n",
      "50 [D loss: 0.105764, acc: 100.00%, op_acc: 60.94%] [G loss: 2.841196]\n",
      "51 [D loss: 0.122766, acc: 98.44%, op_acc: 46.88%] [G loss: 3.495593]\n",
      "52 [D loss: 0.111606, acc: 100.00%, op_acc: 68.75%] [G loss: 3.119571]\n",
      "53 [D loss: 0.066540, acc: 100.00%, op_acc: 68.75%] [G loss: 3.506751]\n",
      "54 [D loss: 0.109273, acc: 98.44%, op_acc: 48.44%] [G loss: 3.329603]\n",
      "55 [D loss: 0.214350, acc: 89.06%, op_acc: 53.12%] [G loss: 3.701120]\n",
      "56 [D loss: 0.097556, acc: 100.00%, op_acc: 75.00%] [G loss: 4.229867]\n",
      "57 [D loss: 0.104395, acc: 98.44%, op_acc: 65.62%] [G loss: 3.068552]\n",
      "58 [D loss: 0.083218, acc: 100.00%, op_acc: 67.19%] [G loss: 2.592863]\n",
      "59 [D loss: 0.070697, acc: 100.00%, op_acc: 70.31%] [G loss: 3.259352]\n",
      "60 [D loss: 0.131637, acc: 98.44%, op_acc: 59.38%] [G loss: 1.747330]\n",
      "61 [D loss: 0.121002, acc: 96.88%, op_acc: 70.31%] [G loss: 2.789020]\n",
      "62 [D loss: 0.133211, acc: 95.31%, op_acc: 59.38%] [G loss: 5.869221]\n",
      "63 [D loss: 0.210924, acc: 81.25%, op_acc: 65.62%] [G loss: 2.436661]\n",
      "64 [D loss: 0.114619, acc: 98.44%, op_acc: 53.12%] [G loss: 3.016586]\n",
      "65 [D loss: 0.228339, acc: 84.38%, op_acc: 45.31%] [G loss: 6.525703]\n",
      "66 [D loss: 1.034488, acc: 14.06%, op_acc: 32.81%] [G loss: 3.967725]\n",
      "67 [D loss: 0.115387, acc: 100.00%, op_acc: 54.69%] [G loss: 4.313765]\n",
      "68 [D loss: 0.215956, acc: 85.94%, op_acc: 50.00%] [G loss: 5.106564]\n",
      "69 [D loss: 0.169143, acc: 93.75%, op_acc: 54.69%] [G loss: 4.535373]\n",
      "70 [D loss: 0.203328, acc: 85.94%, op_acc: 53.12%] [G loss: 2.999665]\n",
      "71 [D loss: 0.118641, acc: 95.31%, op_acc: 56.25%] [G loss: 3.067920]\n",
      "72 [D loss: 0.129410, acc: 93.75%, op_acc: 60.94%] [G loss: 2.577821]\n",
      "73 [D loss: 0.088707, acc: 100.00%, op_acc: 68.75%] [G loss: 2.413583]\n",
      "74 [D loss: 0.220306, acc: 85.94%, op_acc: 51.56%] [G loss: 4.071530]\n",
      "75 [D loss: 0.188129, acc: 92.19%, op_acc: 56.25%] [G loss: 3.493888]\n",
      "76 [D loss: 0.721209, acc: 35.94%, op_acc: 43.75%] [G loss: 3.190104]\n",
      "77 [D loss: 0.421473, acc: 60.94%, op_acc: 37.50%] [G loss: 5.290382]\n",
      "78 [D loss: 0.286918, acc: 81.25%, op_acc: 51.56%] [G loss: 4.154585]\n",
      "79 [D loss: 0.404986, acc: 60.94%, op_acc: 42.19%] [G loss: 3.156890]\n",
      "80 [D loss: 0.350231, acc: 64.06%, op_acc: 51.56%] [G loss: 3.299876]\n",
      "81 [D loss: 0.510863, acc: 53.12%, op_acc: 50.00%] [G loss: 3.083857]\n",
      "82 [D loss: 0.316365, acc: 75.00%, op_acc: 56.25%] [G loss: 3.922608]\n",
      "83 [D loss: 0.329934, acc: 65.62%, op_acc: 53.12%] [G loss: 2.509639]\n",
      "84 [D loss: 0.264629, acc: 76.56%, op_acc: 43.75%] [G loss: 3.230438]\n",
      "85 [D loss: 0.303217, acc: 73.44%, op_acc: 50.00%] [G loss: 2.365044]\n",
      "86 [D loss: 0.424308, acc: 57.81%, op_acc: 42.19%] [G loss: 3.087857]\n",
      "87 [D loss: 0.243598, acc: 84.38%, op_acc: 51.56%] [G loss: 3.498935]\n",
      "88 [D loss: 0.698837, acc: 37.50%, op_acc: 42.19%] [G loss: 2.092777]\n",
      "89 [D loss: 0.198474, acc: 90.62%, op_acc: 57.81%] [G loss: 2.944994]\n",
      "90 [D loss: 0.557301, acc: 51.56%, op_acc: 34.38%] [G loss: 2.585314]\n",
      "91 [D loss: 0.332853, acc: 76.56%, op_acc: 43.75%] [G loss: 2.755433]\n",
      "92 [D loss: 0.335057, acc: 71.88%, op_acc: 45.31%] [G loss: 2.758449]\n",
      "93 [D loss: 0.398435, acc: 62.50%, op_acc: 45.31%] [G loss: 1.874405]\n",
      "94 [D loss: 0.239605, acc: 89.06%, op_acc: 53.12%] [G loss: 2.332880]\n",
      "95 [D loss: 0.224625, acc: 87.50%, op_acc: 37.50%] [G loss: 1.878556]\n",
      "96 [D loss: 0.680062, acc: 43.75%, op_acc: 42.19%] [G loss: 3.290586]\n",
      "97 [D loss: 0.452229, acc: 62.50%, op_acc: 46.88%] [G loss: 2.454451]\n",
      "98 [D loss: 0.618548, acc: 42.19%, op_acc: 45.31%] [G loss: 2.087618]\n",
      "99 [D loss: 0.407326, acc: 54.69%, op_acc: 42.19%] [G loss: 2.203059]\n",
      "100 [D loss: 0.517367, acc: 46.88%, op_acc: 40.62%] [G loss: 2.222279]\n",
      "101 [D loss: 0.447954, acc: 62.50%, op_acc: 39.06%] [G loss: 2.152140]\n",
      "102 [D loss: 0.524373, acc: 56.25%, op_acc: 35.94%] [G loss: 2.396024]\n",
      "103 [D loss: 0.638445, acc: 45.31%, op_acc: 45.31%] [G loss: 2.123188]\n",
      "104 [D loss: 0.669132, acc: 31.25%, op_acc: 39.06%] [G loss: 2.495129]\n",
      "105 [D loss: 0.482223, acc: 56.25%, op_acc: 37.50%] [G loss: 1.989720]\n",
      "106 [D loss: 0.420615, acc: 59.38%, op_acc: 29.69%] [G loss: 1.705915]\n",
      "107 [D loss: 0.482575, acc: 51.56%, op_acc: 42.19%] [G loss: 1.626800]\n",
      "108 [D loss: 0.551966, acc: 43.75%, op_acc: 39.06%] [G loss: 1.707003]\n",
      "109 [D loss: 0.466124, acc: 56.25%, op_acc: 37.50%] [G loss: 2.022039]\n",
      "110 [D loss: 0.480038, acc: 53.12%, op_acc: 40.62%] [G loss: 1.541057]\n",
      "111 [D loss: 0.461777, acc: 53.12%, op_acc: 43.75%] [G loss: 1.731731]\n",
      "112 [D loss: 0.596229, acc: 39.06%, op_acc: 39.06%] [G loss: 1.611165]\n",
      "113 [D loss: 0.509612, acc: 50.00%, op_acc: 39.06%] [G loss: 2.242559]\n",
      "114 [D loss: 0.546757, acc: 45.31%, op_acc: 42.19%] [G loss: 1.701673]\n",
      "115 [D loss: 0.461989, acc: 62.50%, op_acc: 34.38%] [G loss: 1.592299]\n",
      "116 [D loss: 0.504122, acc: 43.75%, op_acc: 42.19%] [G loss: 1.476229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 [D loss: 0.372315, acc: 60.94%, op_acc: 35.94%] [G loss: 1.789702]\n",
      "118 [D loss: 0.555577, acc: 53.12%, op_acc: 39.06%] [G loss: 1.851548]\n",
      "119 [D loss: 0.416960, acc: 51.56%, op_acc: 46.88%] [G loss: 2.325421]\n",
      "120 [D loss: 0.458810, acc: 56.25%, op_acc: 39.06%] [G loss: 1.901093]\n",
      "121 [D loss: 0.372144, acc: 56.25%, op_acc: 42.19%] [G loss: 1.656196]\n",
      "122 [D loss: 0.353309, acc: 73.44%, op_acc: 35.94%] [G loss: 2.038827]\n",
      "123 [D loss: 0.365607, acc: 65.62%, op_acc: 43.75%] [G loss: 2.030883]\n",
      "124 [D loss: 0.550374, acc: 54.69%, op_acc: 42.19%] [G loss: 1.397941]\n",
      "125 [D loss: 0.423733, acc: 54.69%, op_acc: 35.94%] [G loss: 1.631450]\n",
      "126 [D loss: 0.405314, acc: 57.81%, op_acc: 46.88%] [G loss: 1.935527]\n",
      "127 [D loss: 0.498778, acc: 54.69%, op_acc: 42.19%] [G loss: 1.894902]\n",
      "128 [D loss: 0.435021, acc: 54.69%, op_acc: 39.06%] [G loss: 1.475086]\n",
      "129 [D loss: 0.321941, acc: 75.00%, op_acc: 50.00%] [G loss: 1.981306]\n",
      "130 [D loss: 0.441993, acc: 65.62%, op_acc: 43.75%] [G loss: 1.595490]\n",
      "131 [D loss: 0.453841, acc: 64.06%, op_acc: 32.81%] [G loss: 1.865797]\n",
      "132 [D loss: 0.529644, acc: 45.31%, op_acc: 29.69%] [G loss: 1.668015]\n",
      "133 [D loss: 0.435816, acc: 57.81%, op_acc: 43.75%] [G loss: 1.780939]\n",
      "134 [D loss: 0.407995, acc: 62.50%, op_acc: 40.62%] [G loss: 1.445599]\n",
      "135 [D loss: 0.549145, acc: 50.00%, op_acc: 32.81%] [G loss: 1.172090]\n",
      "136 [D loss: 0.414418, acc: 59.38%, op_acc: 40.62%] [G loss: 1.706387]\n",
      "137 [D loss: 0.392291, acc: 59.38%, op_acc: 45.31%] [G loss: 2.058756]\n",
      "138 [D loss: 0.444656, acc: 51.56%, op_acc: 42.19%] [G loss: 1.691043]\n",
      "139 [D loss: 0.333811, acc: 70.31%, op_acc: 45.31%] [G loss: 1.558419]\n",
      "140 [D loss: 0.483288, acc: 48.44%, op_acc: 34.38%] [G loss: 1.185211]\n",
      "141 [D loss: 0.330699, acc: 71.88%, op_acc: 43.75%] [G loss: 1.804334]\n",
      "142 [D loss: 0.507791, acc: 48.44%, op_acc: 39.06%] [G loss: 1.678087]\n",
      "143 [D loss: 0.429425, acc: 53.12%, op_acc: 42.19%] [G loss: 1.602779]\n",
      "144 [D loss: 0.445862, acc: 54.69%, op_acc: 43.75%] [G loss: 1.387492]\n",
      "145 [D loss: 0.394876, acc: 59.38%, op_acc: 46.88%] [G loss: 1.630683]\n",
      "146 [D loss: 0.433468, acc: 57.81%, op_acc: 42.19%] [G loss: 1.584121]\n",
      "147 [D loss: 0.418460, acc: 62.50%, op_acc: 43.75%] [G loss: 1.275617]\n",
      "148 [D loss: 0.436499, acc: 48.44%, op_acc: 45.31%] [G loss: 1.600354]\n",
      "149 [D loss: 0.463955, acc: 56.25%, op_acc: 43.75%] [G loss: 1.668536]\n",
      "150 [D loss: 0.418176, acc: 67.19%, op_acc: 42.19%] [G loss: 1.561025]\n",
      "151 [D loss: 0.392855, acc: 59.38%, op_acc: 48.44%] [G loss: 1.499097]\n",
      "152 [D loss: 0.329401, acc: 68.75%, op_acc: 37.50%] [G loss: 1.716707]\n",
      "153 [D loss: 0.504768, acc: 40.62%, op_acc: 43.75%] [G loss: 1.508623]\n",
      "154 [D loss: 0.442515, acc: 62.50%, op_acc: 50.00%] [G loss: 1.330413]\n",
      "155 [D loss: 0.420135, acc: 60.94%, op_acc: 46.88%] [G loss: 1.417690]\n",
      "156 [D loss: 0.427004, acc: 51.56%, op_acc: 45.31%] [G loss: 1.836058]\n",
      "157 [D loss: 0.414897, acc: 56.25%, op_acc: 48.44%] [G loss: 1.733183]\n",
      "158 [D loss: 0.394344, acc: 59.38%, op_acc: 43.75%] [G loss: 1.253509]\n",
      "159 [D loss: 0.386540, acc: 62.50%, op_acc: 42.19%] [G loss: 1.724831]\n",
      "160 [D loss: 0.447558, acc: 50.00%, op_acc: 50.00%] [G loss: 1.451862]\n",
      "161 [D loss: 0.406958, acc: 57.81%, op_acc: 48.44%] [G loss: 1.704397]\n",
      "162 [D loss: 0.407646, acc: 67.19%, op_acc: 37.50%] [G loss: 2.048129]\n",
      "163 [D loss: 0.399351, acc: 64.06%, op_acc: 51.56%] [G loss: 1.535807]\n",
      "164 [D loss: 0.350411, acc: 65.62%, op_acc: 62.50%] [G loss: 1.577544]\n",
      "165 [D loss: 0.391517, acc: 60.94%, op_acc: 48.44%] [G loss: 1.552860]\n",
      "166 [D loss: 0.386308, acc: 59.38%, op_acc: 51.56%] [G loss: 1.574256]\n",
      "167 [D loss: 0.478261, acc: 51.56%, op_acc: 51.56%] [G loss: 1.207220]\n",
      "168 [D loss: 0.369679, acc: 64.06%, op_acc: 32.81%] [G loss: 1.507759]\n",
      "169 [D loss: 0.348689, acc: 65.62%, op_acc: 42.19%] [G loss: 1.621306]\n",
      "170 [D loss: 0.354175, acc: 71.88%, op_acc: 40.62%] [G loss: 1.627271]\n",
      "171 [D loss: 0.406044, acc: 62.50%, op_acc: 45.31%] [G loss: 1.773199]\n",
      "172 [D loss: 0.365104, acc: 57.81%, op_acc: 48.44%] [G loss: 1.392428]\n",
      "173 [D loss: 0.375436, acc: 60.94%, op_acc: 40.62%] [G loss: 1.489869]\n",
      "174 [D loss: 0.443947, acc: 54.69%, op_acc: 43.75%] [G loss: 1.299930]\n",
      "175 [D loss: 0.272904, acc: 78.12%, op_acc: 53.12%] [G loss: 1.793958]\n",
      "176 [D loss: 0.426351, acc: 60.94%, op_acc: 37.50%] [G loss: 1.482644]\n",
      "177 [D loss: 0.392800, acc: 62.50%, op_acc: 48.44%] [G loss: 1.591371]\n",
      "178 [D loss: 0.305109, acc: 71.88%, op_acc: 57.81%] [G loss: 1.416318]\n",
      "179 [D loss: 0.410007, acc: 54.69%, op_acc: 53.12%] [G loss: 1.672878]\n",
      "180 [D loss: 0.412918, acc: 65.62%, op_acc: 54.69%] [G loss: 1.596429]\n",
      "181 [D loss: 0.372165, acc: 65.62%, op_acc: 42.19%] [G loss: 1.795200]\n",
      "182 [D loss: 0.404025, acc: 56.25%, op_acc: 50.00%] [G loss: 1.712020]\n",
      "183 [D loss: 0.394749, acc: 57.81%, op_acc: 46.88%] [G loss: 1.478903]\n",
      "184 [D loss: 0.394810, acc: 59.38%, op_acc: 48.44%] [G loss: 1.695218]\n",
      "185 [D loss: 0.334657, acc: 67.19%, op_acc: 45.31%] [G loss: 1.790659]\n",
      "186 [D loss: 0.417575, acc: 50.00%, op_acc: 51.56%] [G loss: 1.591938]\n",
      "187 [D loss: 0.420346, acc: 53.12%, op_acc: 48.44%] [G loss: 1.408770]\n",
      "188 [D loss: 0.362413, acc: 68.75%, op_acc: 40.62%] [G loss: 1.553840]\n",
      "189 [D loss: 0.393342, acc: 65.62%, op_acc: 42.19%] [G loss: 1.753248]\n",
      "190 [D loss: 0.443482, acc: 54.69%, op_acc: 42.19%] [G loss: 1.504435]\n",
      "191 [D loss: 0.429624, acc: 50.00%, op_acc: 48.44%] [G loss: 1.270502]\n",
      "192 [D loss: 0.391369, acc: 62.50%, op_acc: 45.31%] [G loss: 1.567779]\n",
      "193 [D loss: 0.391520, acc: 53.12%, op_acc: 51.56%] [G loss: 1.581586]\n",
      "194 [D loss: 0.461907, acc: 53.12%, op_acc: 37.50%] [G loss: 1.423110]\n",
      "195 [D loss: 0.447118, acc: 45.31%, op_acc: 37.50%] [G loss: 1.236879]\n",
      "196 [D loss: 0.367363, acc: 64.06%, op_acc: 35.94%] [G loss: 1.263684]\n",
      "197 [D loss: 0.461670, acc: 51.56%, op_acc: 42.19%] [G loss: 1.740589]\n",
      "198 [D loss: 0.377511, acc: 62.50%, op_acc: 43.75%] [G loss: 1.429109]\n",
      "199 [D loss: 0.373260, acc: 60.94%, op_acc: 48.44%] [G loss: 1.461657]\n",
      "200 [D loss: 0.412696, acc: 65.62%, op_acc: 51.56%] [G loss: 1.163002]\n",
      "201 [D loss: 0.351179, acc: 67.19%, op_acc: 45.31%] [G loss: 1.704687]\n",
      "202 [D loss: 0.407639, acc: 54.69%, op_acc: 37.50%] [G loss: 1.554009]\n",
      "203 [D loss: 0.312299, acc: 67.19%, op_acc: 46.88%] [G loss: 1.458224]\n",
      "204 [D loss: 0.405804, acc: 56.25%, op_acc: 50.00%] [G loss: 1.506678]\n",
      "205 [D loss: 0.434819, acc: 54.69%, op_acc: 42.19%] [G loss: 1.353789]\n",
      "206 [D loss: 0.415574, acc: 57.81%, op_acc: 37.50%] [G loss: 1.537241]\n",
      "207 [D loss: 0.436757, acc: 48.44%, op_acc: 39.06%] [G loss: 1.373438]\n",
      "208 [D loss: 0.432212, acc: 51.56%, op_acc: 46.88%] [G loss: 1.375407]\n",
      "209 [D loss: 0.386571, acc: 59.38%, op_acc: 46.88%] [G loss: 1.177504]\n",
      "210 [D loss: 0.404627, acc: 65.62%, op_acc: 34.38%] [G loss: 1.138111]\n",
      "211 [D loss: 0.363348, acc: 62.50%, op_acc: 42.19%] [G loss: 1.540746]\n",
      "212 [D loss: 0.379112, acc: 65.62%, op_acc: 42.19%] [G loss: 1.563041]\n",
      "213 [D loss: 0.478914, acc: 50.00%, op_acc: 42.19%] [G loss: 1.451043]\n",
      "214 [D loss: 0.353074, acc: 65.62%, op_acc: 53.12%] [G loss: 1.873165]\n",
      "215 [D loss: 0.347085, acc: 65.62%, op_acc: 46.88%] [G loss: 1.742638]\n",
      "216 [D loss: 0.379388, acc: 59.38%, op_acc: 39.06%] [G loss: 1.430216]\n",
      "217 [D loss: 0.425197, acc: 60.94%, op_acc: 39.06%] [G loss: 1.302177]\n",
      "218 [D loss: 0.261693, acc: 81.25%, op_acc: 53.12%] [G loss: 1.466959]\n",
      "219 [D loss: 0.422983, acc: 51.56%, op_acc: 53.12%] [G loss: 1.455370]\n",
      "220 [D loss: 0.350350, acc: 65.62%, op_acc: 43.75%] [G loss: 1.484752]\n",
      "221 [D loss: 0.431904, acc: 56.25%, op_acc: 48.44%] [G loss: 1.197698]\n",
      "222 [D loss: 0.469703, acc: 57.81%, op_acc: 35.94%] [G loss: 1.352050]\n",
      "223 [D loss: 0.400684, acc: 54.69%, op_acc: 45.31%] [G loss: 1.613311]\n",
      "224 [D loss: 0.399805, acc: 54.69%, op_acc: 53.12%] [G loss: 1.306157]\n",
      "225 [D loss: 0.402505, acc: 56.25%, op_acc: 51.56%] [G loss: 1.500187]\n",
      "226 [D loss: 0.409821, acc: 51.56%, op_acc: 56.25%] [G loss: 1.402153]\n",
      "227 [D loss: 0.404410, acc: 56.25%, op_acc: 48.44%] [G loss: 1.437176]\n",
      "228 [D loss: 0.371311, acc: 57.81%, op_acc: 42.19%] [G loss: 1.190257]\n",
      "229 [D loss: 0.352791, acc: 64.06%, op_acc: 54.69%] [G loss: 1.444384]\n",
      "230 [D loss: 0.413170, acc: 57.81%, op_acc: 45.31%] [G loss: 1.287820]\n",
      "231 [D loss: 0.400305, acc: 64.06%, op_acc: 43.75%] [G loss: 1.713646]\n",
      "232 [D loss: 0.354415, acc: 70.31%, op_acc: 43.75%] [G loss: 1.283487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 [D loss: 0.411657, acc: 53.12%, op_acc: 45.31%] [G loss: 1.269415]\n",
      "234 [D loss: 0.380746, acc: 60.94%, op_acc: 43.75%] [G loss: 1.637947]\n",
      "235 [D loss: 0.469757, acc: 53.12%, op_acc: 45.31%] [G loss: 1.532785]\n",
      "236 [D loss: 0.390993, acc: 59.38%, op_acc: 50.00%] [G loss: 1.605288]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "            loss_weights=[0.5, 0.5],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        noise = Input(shape=(100,))\n",
    "        img = self.generator(noise)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines valid\n",
    "        valid, _ = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(noise , valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        features = model(img)\n",
    "        valid = Dense(1, activation=\"sigmoid\")(features)\n",
    "        label = Dense(self.num_classes+1, activation=\"softmax\")(features)\n",
    "\n",
    "        return Model(img, [valid, label])\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Class weights:\n",
    "        # To balance the difference in occurences of digit class labels.\n",
    "        # 50% of labels that the discriminator trains on are 'fake'.\n",
    "        # Weight = 1 / frequency\n",
    "        cw1 = {0: 1, 1: 1}\n",
    "        cw2 = {i: self.num_classes / 64 for i in range(self.num_classes)}\n",
    "        cw2[self.num_classes] = 1 / 64\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # One-hot encoding of labels\n",
    "            labels = to_categorical(y_train[idx], num_classes=self.num_classes+1)\n",
    "            fake_labels = to_categorical(np.full((batch_size, 1), self.num_classes), num_classes=self.num_classes+1)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, labels], class_weight=[cw1, cw2])\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels], class_weight=[cw1, cw2])\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid, class_weight=[cw1, cw2])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 1\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"mnist_sgan_generator\")\n",
    "        save(self.discriminator, \"mnist_sgan_discriminator\")\n",
    "        save(self.combined, \"mnist_sgan_adversarial\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sgan = SGAN()\n",
    "    sgan.train(epochs=20000, batch_size=32, sample_interval=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
